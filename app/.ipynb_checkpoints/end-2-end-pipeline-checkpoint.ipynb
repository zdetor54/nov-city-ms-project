{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lookup candidate entities and classes\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--input_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'data'),\n",
    "    help='Directory of input/output')\n",
    "parser.add_argument(\n",
    "    '--file_type',\n",
    "    type=str,\n",
    "    default='csv',\n",
    "    help='File type')\n",
    "parser.add_argument(\n",
    "    '--lookup_results_rank',\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help='File type')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "# if not os.path.exists(FLAGS.input_dir):\n",
    "#     os.mkdir(FLAGS.input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files from the input directory\n",
    "def get_data_files(data_folder):\n",
    "    \"\"\"\n",
    "    A function used to get all the csv files from the input directory\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        the folder within  the working directory where the data is located\n",
    "    \"\"\"\n",
    "\n",
    "    files = [] # a list of all filenames, including file extensions, that contain data\n",
    "    csv_files = [] # same list as above but without the file extension\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    csv_files = [f.replace(\".csv\",\"\") for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def get_target_cta_columns(target_config_file, data_folder, csv_files, filter_col = True):\n",
    "    \"\"\"\n",
    "    A function used to get which columns from the csv files need to be considered for the CTA. This is a subset of the file columns ignoring anything that is not an entity\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_config_file : str\n",
    "        the file that contains the target column indices for each file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    filter_col : boolean\n",
    "        a flag to indicate whether we should narrow down the reading of the columns to only those targeted for the CTA task\n",
    "    \"\"\"\n",
    "   \n",
    "    target_col_file = os.path.join(FLAGS.input_dir+data_folder, target_config_file)\n",
    "    df_target_col = pd.read_csv(target_col_file,header=None, names=['filename','column_index'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_target_col = df_target_col.loc[df_target_col['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_target = dict()\n",
    "    \n",
    "    for index,row in df_target_col.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_target:\n",
    "            dict_target[row['filename']]= []\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        if filter_col:\n",
    "            dict_target[row['filename']].append(int(row['column_index']))\n",
    "    \n",
    "    return dict_target\n",
    "\n",
    "def get_ground_truth(file, folder, csv_files):\n",
    "    \"\"\"\n",
    "    A function used to get the ground truths as provided in the setup\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file : str\n",
    "        the file that contains the ground truth for the class of each column in each file\n",
    "    folder : str\n",
    "        the folder that contains the ground truth file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    \"\"\"\n",
    "    \n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "   \n",
    "    filepath = os.path.join(FLAGS.input_dir+folder, file)\n",
    "    df_ground_truth = pd.read_csv(filepath,header=None, names=['filename','column_index', 'class'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_ground_truth = df_ground_truth.loc[df_ground_truth['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_gt = dict()\n",
    "    \n",
    "    for index,row in df_ground_truth.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_gt:\n",
    "            dict_gt[row['filename']]= dict()\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        dict_gt[row['filename']][int(row['column_index'])] = row['class'].split(dbo_prefix)[1]\n",
    "    \n",
    "    return dict_gt\n",
    "\n",
    "def read_data(data_folder, dict_target_col, has_header_row = False):\n",
    "    \"\"\"\n",
    "    A function used to read the data from the csvs in the data_folder only considering the columns that are in the dict_target_col\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    folder : str\n",
    "        the folder that contains the csvs with the tabular data\n",
    "    dict_target_col : dictionary\n",
    "        a dictionary with csv filenames as the key and an array of relevant column indices as a value\n",
    "    has_header_row : boolean\n",
    "        a flag to indicate whether the first row in the csv files needs to be skipped as it is a header\n",
    "    \"\"\"\n",
    "    data = list()\n",
    "\n",
    "    for file in dict_target_col:\n",
    "        element = dict()\n",
    "        element['filename'] = file\n",
    "        df_data = pd.DataFrame()\n",
    "        df_title = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        filename = file + '.' + FLAGS.file_type\n",
    "        tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "\n",
    "        # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "        if len(dict_target_col[file])>0:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "        else:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "        # add the column headers to the data dictionary\n",
    "        try:\n",
    "            element['column_titles'] = list(df_title.iloc[0,:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        file_element = dict()\n",
    "        for column in df_data.columns:\n",
    "            file_element[column] = list(set(df_data[column])) #SEE IF WE CAN COMMENT THE SET OUT AND AGE ALL THE DATA IN WITH REPETITION\n",
    "        element['data'] = file_element\n",
    "\n",
    "#         element['dataframe'] = df_data    \n",
    "        data.append(element)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Setup\n",
    "\n",
    "As part of this initial step we will need to load the data we are going to process as well as the targets we are trying to meet. The data is located in the data folder as follows\n",
    "- round_1:\n",
    "    - gt: the expected outcome (ground truth)\n",
    "    - tables: the tabular data\n",
    "    - targets: the columns / cells we need to consider for the CTA/CEA\n",
    "----\n",
    "Step 1: Get a list of all the csv files in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10579449_0_1681126353774891032',\n",
       " '11833461_1_3811022039809817402',\n",
       " '13719111_1_5719401842463579519',\n",
       " '14067031_0_559833072073397908',\n",
       " '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of csv files with tabular data\n",
    "csv_files = get_data_files('\\\\round_1\\\\tables')\n",
    "# csv_files = csv_files[:1]\n",
    "csv_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get the columns we need to consider for the CTA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('58891288_0_1117541047012405958', [1, 3]),\n",
       " ('8468806_0_4382447409703007384', [1, 2]),\n",
       " ('50245608_0_871275842592178099', [0, 3, 4]),\n",
       " ('14067031_0_559833072073397908', [1, 7, 5, 0]),\n",
       " ('39759273_0_1427898308030295194', [1, 3])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns we need to consider for the CTA task\n",
    "dict_target_col = get_target_cta_columns('CTA_Round1_Targets.csv', '\\\\round_1\\\\targets', csv_files,True)\n",
    "list(islice(dict_target_col.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the ground truth for all columns in the set of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('58891288_0_1117541047012405958', {1: 'Film', 3: 'Person'}),\n",
       " ('8468806_0_4382447409703007384', {1: 'Lake', 2: 'Country'}),\n",
       " ('50245608_0_871275842592178099', {0: 'Film', 3: 'Person', 4: 'Writer'}),\n",
       " ('14067031_0_559833072073397908',\n",
       "  {1: 'Language', 7: 'Currency', 5: 'City', 0: 'Country'}),\n",
       " ('39759273_0_1427898308030295194', {1: 'Film', 3: 'Person'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = get_ground_truth('CTA_Round1_gt.csv', '\\\\round_1\\\\gt', csv_files)\n",
    "list(islice(ground_truth.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "The next step is to load the data from the csv files. We load the data as an array of dictionaries.\n",
    "Each dictionary will have the following structure:<br>\n",
    "{<br>\n",
    "<blockquote>\n",
    "<strong>'filename':</strong> '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5',<br>\n",
    "<strong>'column_titles'</strong>: ['Party'],<br>\n",
    "<strong>'data'</strong>: <br>\n",
    "    {<br>\n",
    "    <blockquote>\n",
    "        <strong>0:</strong> ['PC', 'Lib-Dem','SNP','UKIP','Labour','BNP','Conservative','Green']<br>\n",
    "    </blockquote>\n",
    "        }<br>\n",
    "</blockquote>    \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('\\\\round_1\\\\tables', dict_target_col,True)\n",
    "\n",
    "def append_gt_to_data(data, ground_truth):\n",
    "    for file in data:\n",
    "        filename = file['filename']\n",
    "    #     print(filename)\n",
    "        file['gt'] = dict() \n",
    "        for col in file['data']:\n",
    "    #         print(col, ground_truth[filename][col])\n",
    "            file['gt'][col] = ground_truth[filename][col]\n",
    "        \n",
    "append_gt_to_data(data, ground_truth)\n",
    "\n",
    "with open(('data-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in data:\n",
    "    filename = file['filename']\n",
    "#     print(filename)\n",
    "    file['gt'] = dict() \n",
    "    for col in file['data']:\n",
    "#         print(col, ground_truth[filename][col])\n",
    "        file['gt'][col] = ground_truth[filename][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbo_sparql_results(query_string):\n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    sparql.setQuery(query_string)\n",
    "    \n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        return qres\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_dbo_classes_sparql(cell):\n",
    "    \n",
    "    classes = list([])\n",
    "\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "#     print(f'###################{cell}########################')\n",
    "    query_string = f'''\n",
    "    SELECT ?class\n",
    "    WHERE {{ dbr:{cell} a ?class.\n",
    "    }}'''\n",
    "\n",
    "#         query_string = f'''\n",
    "#         select distinct ?superclass \n",
    "#         where {{dbr:{cell} rdf:type ?e. \n",
    "#             ?e rdfs:subClassOf* ?superclass.\n",
    "#         FILTER (strstarts(str(?superclass), '{dbo_prefix}'))}}'''\n",
    "\n",
    "#         print(query_string)\n",
    "\n",
    "    qres = dbo_sparql_results(query_string)\n",
    "#         pprint(qres)\n",
    "    try:\n",
    "        for entity_class in qres['results']['bindings']:\n",
    "            if dbo_prefix in entity_class[list(qres['results']['bindings'][1].keys())[0]]['value']:\n",
    "                candicate_class = entity_class[list(qres['results']['bindings'][1].keys())[0]]['value'].split(dbo_prefix)[1]\n",
    "                classes.append(candicate_class)\n",
    "#                 print(candicate_class)\n",
    "    except:\n",
    "        pass\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_dbpedia_classes (query_string, max_hits = 5):\n",
    "    web_api = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=%s&QueryString=%s'\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    entity_classes = dict()\n",
    "    try:\n",
    "        lookup_url = web_api % (max_hits, query_string)\n",
    "        print(lookup_url)\n",
    "#         print(lookup_url)\n",
    "        lookup_res = requests.get(lookup_url)\n",
    "        root = ET.fromstring(lookup_res.content)\n",
    "        i=0\n",
    "        for child in root:\n",
    "            i+=1\n",
    "#             print(\"\\n\")\n",
    "            entity = child[1].text.split(dbp_prefix)[1]\n",
    "#             print(entity)\n",
    "            classes = list()\n",
    "            for cc in child[3]:\n",
    "                cls_URI = cc[1].text\n",
    "#                 print(cls_URI)\n",
    "                if dbo_prefix in cls_URI:\n",
    "                    classes.append((cls_URI.split(dbo_prefix)[1]))\n",
    "            \n",
    "            # if no classes have been retrieved from the lookup go to the sparql endpoint to get the classes for the entity\n",
    "            if len(classes) == 0:\n",
    "                classes = get_dbo_classes_sparql(re.escape(entity))\n",
    "                \n",
    "            if len(classes) > 0:\n",
    "                entity_classes[entity] = dict()\n",
    "                entity_classes[entity]['rank'] = i\n",
    "                entity_classes[entity]['candidate_classes'] = classes\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return entity_classes\n",
    "\n",
    "def lookup_cells_in_dbpedia(data):\n",
    "    cell_values = dict()\n",
    "    i = 0\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    size = 0\n",
    "    for file_i in range(len(data)):\n",
    "        for col in data[file_i]['data']:\n",
    "            for line_j in range(len(data[file_i]['data'][col])):\n",
    "                size+=1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # from tqdm import tqdm\n",
    "    for file_i in range(len(data)):\n",
    "    #     print(data[file_i])\n",
    "        filename = data[file_i]['filename']\n",
    "        for col in data[file_i]['data']:\n",
    "            column_index = col\n",
    "    #         print(col)\n",
    "    #         print(data[file_i]['data'][col])\n",
    "            for line_j in range(len(data[file_i]['data'][col])):\n",
    "                i+=1\n",
    "                cell_value = data[file_i]['data'][col][line_j]\n",
    "                clear_output(wait=True)\n",
    "                print('{0:.2f}'.format(100*i/size,2),'-->',filename, \": \",cell_value)\n",
    "                if cell_value in cell_values.keys():\n",
    "                    cell_values[cell_value]['location'].append((filename,column_index))\n",
    "                else:\n",
    "                    cell_values[cell_value] = dict()\n",
    "                    cell_values[cell_value]['location'] = [(filename,column_index)]\n",
    "                    try:\n",
    "                        cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value.replace(\"[\",'').replace(\"]\",''),FLAGS.lookup_results_rank)\n",
    "                    except:\n",
    "                        cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value,FLAGS.lookup_results_rank)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"{int(end_time - start_time)//60} min and {int((end_time - start_time)%60)} seconds Elapsed\")\n",
    "    \n",
    "    # Also save the data in json for future runs\n",
    "    with open(('cell_values-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(cell_values, fp)\n",
    "    \n",
    "    return cell_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "from pprint import pprint\n",
    "\n",
    "def get_dbo_class_entities_sparql(candidate_class, num_of_results = 10000):\n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    \n",
    "    ent_list = []\n",
    "\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    \n",
    "#     print(f'###################{candidate_class}########################')\n",
    "    sparql.setQuery(f'''\n",
    "    SELECT ?object\n",
    "    WHERE {{ ?object a dbo:{candidate_class}. }}\n",
    "    ORDER BY RAND()\n",
    "    LIMIT {num_of_results}\n",
    "    ''')\n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        for entity in qres['results']['bindings']:\n",
    "            ent_list.append(entity['object']['value'].split(dbp_prefix)[1])\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return ent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lookup cell_values\n",
    "\n",
    "With the data loaded in the *data* dictionary the next step is to lookup the cell values in the DBpedia endpoint and get the canidate classes and entities.\n",
    "Each cell value is only looked up once, however we still keep track of any column it might have appeared in as well as all candidate entities and classes it may have matched to.\n",
    "\n",
    "For this level of analysis we are flexible to store the 5 top lookup results for each cell value (default value for FLAGS.lookup_results_rank).\n",
    "We will then assess the number of classifiers we need to train later and perhaps filter out any candidate classes that only appeared in lower ranks.\n",
    "\n",
    "The outcome of the lookup is stored in the *cell_values* dictionary as follows:\n",
    "\n",
    "{<strong>\"Madagascar\":</strong><br>\n",
    "{\n",
    "<blockquote><strong>\"location\":</strong> [(\"14067031_0_559833072073397908\",0)]\n",
    "            , <br><strong>\"candidate_entities\":</strong><br> \n",
    "                        {\n",
    "                            <blockquote><strong>\"Madagascar\":</strong> <br>{<blockquote><strong>\"rank\":</strong> 1,<br> <strong>\"candidate_classes\":</strong> [\"Place\", \"Country\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"Antananarivo\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 3,<br> <strong>\"candidate_classes\":</strong> [\"Settlement\", \"Place\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"List_of_Madagascar_(franchise)_characters\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 4,<br> <strong>\"candidate_classes\":</strong> [\"FictionalCharacter\", \"Agent\"]</blockquote>},<br>\n",
    "                            <strong>\"Madagascar_national_football_team\"</strong> <br> {<blockquote><strong>\"rank\":</strong> 5,<br> <strong>\"candidate_classes\":</strong> [\"Organisation\", \"SoccerClub\", \"Agent\", \"SportsClub\"]</blockquote>}<br>\n",
    "</blockquote>}<br> \n",
    "</blockquote>},<br>\n",
    "               \n",
    " <strong>\"South Africa\":</strong> {...},<br>\n",
    "  ...<br>\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE NOTE THAT THIS STEP <span style=\"color:red\">TAKES A LONG TIME TO RUN</span>.\n",
    "IF THERE IS ALREADY A CELL_VALUES.JSON FROM A PREVIOUS RUN THEN THAT SHOULD BE LOADED INSTEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively load the lookup values previously saved as part of a past lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "\n",
    "cell_values_directory = os.getcwd()+'\\\\output\\\\'\n",
    "\n",
    "try:\n",
    "    cell_values_json = cell_values_directory + 'cell_values.json'\n",
    "    with open(cell_values_json) as json_file:\n",
    "        cell_values = json.load(json_file)\n",
    "except:\n",
    "    cell_values = lookup_cells_in_dbpedia(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(islice(cell_values.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_dbpedia_classes('Indiana Jones',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process that data\n",
    "\n",
    "The next step is to process the data so that we can use them for training the classifiers and also predicting classes. To achieve that we create the following structures:\n",
    "\n",
    "## 3.1 dict_col_candidate_classes\n",
    "This is a dictionary with the following structure <br/>\n",
    "{<strong>'58891288_0_1117541047012405958'</strong>: \n",
    "<br/>{\n",
    "<blockquote><strong>1</strong>: [('PoliticalParty', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                   ('Organisation', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                   ('Agent', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                    ...<br/>\n",
    "                   ('Book', 'The_Bridge_over_the_River_Kwai', 'The Bridge on the River Kwai', 1)]<br>\n",
    "            <strong>2</strong>: [('PoliticalParty', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                    ...<br/>\n",
    "</blockquote>\n",
    "}<br/>\n",
    "<strong>'58891288_0_1117541047012405958'</strong>: {...}<br/>\n",
    "}\n",
    "where each element in the array represents (type, entity, cell value, rank) of all the lookup results for each cell in that column of that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_candidate_classes = dict()\n",
    "threshold = 2\n",
    "\n",
    "for filename in dict_target_col: #later replace with dict_target_col\n",
    "    dict_col_candidate_classes[filename] = dict()\n",
    "    for i in dict_target_col[filename]:\n",
    "        dict_col_candidate_classes[filename][i] = []\n",
    "#     print(key)\n",
    "    for cell_value in cell_values:\n",
    "        try:\n",
    "            column_index = dict(cell_values[cell_value]['location'])[filename]\n",
    "            for candidate_entity in cell_values[cell_value]['candidate_entities']:\n",
    "#                 print(candidate_entity)\n",
    "                rank = cell_values[cell_value]['candidate_entities'][candidate_entity]['rank']\n",
    "                if rank <= threshold:\n",
    "                    for candidate_class in cell_values[cell_value]['candidate_entities'][candidate_entity]['candidate_classes']:\n",
    "                        dict_col_candidate_classes[filename][column_index].append((candidate_class, candidate_entity, cell_value,rank))\n",
    "                \n",
    "            dict_col_candidate_classes[filename][column_index]\n",
    "#             print('found')\n",
    "        except:\n",
    "            pass\n",
    "#             print('not found')\n",
    "\n",
    "\n",
    "with open(('dict_col_candidate_classes-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_col_candidate_classes, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the file columns to do some sample testing\n",
    "from itertools import islice\n",
    "\n",
    "# list(islice(dict_col_candidate_classes.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the effectiveness of the simple lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.74% of the columns have the right type in the top 10 candidate classes\n"
     ]
    }
   ],
   "source": [
    "# Test the effectiveness of the simple lookup\n",
    "\n",
    "def lookup_assessment(dict_col_candidate_classes, threshold = 10000):\n",
    "    found = 0\n",
    "    total_columns = 0\n",
    "    for file in ground_truth:\n",
    "        for col in ground_truth[file]:\n",
    "            actual_cls = ground_truth[file][col]\n",
    "            candidate_cls = Counter([i[0] for i in dict_col_candidate_classes[file][col]]).most_common()[:threshold]\n",
    "    #         print (file, col, actual_cls, candidate_cls)\n",
    "            if actual_cls in dict(candidate_cls):\n",
    "                found+=1\n",
    "            total_columns+=1\n",
    "    print(f\"{round(100*found/total_columns,2)}% of the columns have the right type in{(' the full list of', ' the top ' + str(threshold))[threshold != 10000] } candidate classes\")\n",
    "\n",
    "lookup_assessment(dict_col_candidate_classes, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 df_entities\n",
    "\n",
    "A variariation of dict_col_candidate_classes this is a datafra of the lookup results with columns representing:\n",
    "* type\n",
    "* entity\n",
    "* cell_value and \n",
    "* rank\n",
    "\n",
    "of all the lookup results regardless of file / column the cell value appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>entity</th>\n",
       "      <th>cell_value</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Film</td>\n",
       "      <td>Night_Hunter</td>\n",
       "      <td>The Night of the Hunter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Album</td>\n",
       "      <td>I'm_Breathless</td>\n",
       "      <td>Breathless</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MusicalWork</td>\n",
       "      <td>I'm_Breathless</td>\n",
       "      <td>Breathless</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Work</td>\n",
       "      <td>I'm_Breathless</td>\n",
       "      <td>Breathless</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Album</td>\n",
       "      <td>American_Beauty/American_Psycho</td>\n",
       "      <td>American Beauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                           entity               cell_value  rank\n",
       "0         Film                     Night_Hunter  The Night of the Hunter     2\n",
       "1        Album                   I'm_Breathless               Breathless     2\n",
       "2  MusicalWork                   I'm_Breathless               Breathless     2\n",
       "3         Work                   I'm_Breathless               Breathless     2\n",
       "4        Album  American_Beauty/American_Psycho          American Beauty     1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type_neighours_pos_neg_samples['Embryology']\n",
    "df_entities = pd.DataFrame()\n",
    "\n",
    "for filename in dict_col_candidate_classes:\n",
    "    for col in dict_col_candidate_classes[filename]:\n",
    "        df_entities = df_entities.append(pd.DataFrame(dict_col_candidate_classes[filename][col], columns=['type', 'entity', 'cell_value', 'rank']))\n",
    "        \n",
    "df_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 type_neighours_pos_neg_samples\n",
    "\n",
    "This is a dictionary that will help create classifiers for the the candidate classes. For each identified class this dictionary will have the following keys:\n",
    "* 'cooccuring_classes': a set of classes that appear in the same columns as this class\n",
    "* 'positive_candidate_entities': a set of all positive candidate entities that have been retrieved from lookups of the tabular data and belong to this class\n",
    "* 'negative_candidate_entities': a set of all negative candidate entities that have been retrieved from lookups of the tabular data and belong to any of the classes in the neighborhood of this one\n",
    "\n",
    "### 3.2.1 Create the dictionary with all classes and populate the coocuring classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_classes(dict_col_candidate_classes):\n",
    "    \n",
    "    candidate_classes = dict()\n",
    "    for file in dict_col_candidate_classes:\n",
    "        for col in (dict_col_candidate_classes[file]):\n",
    "            \n",
    "            neighours = set([])\n",
    "            for cell in dict_col_candidate_classes[file][col]:\n",
    "                neighours.add(cell[0])\n",
    "                if cell[0] not in candidate_classes.keys():\n",
    "                    candidate_classes[cell[0]] = dict()\n",
    "                    candidate_classes[cell[0]]['cooccuring_classes'] = set()\n",
    "                    candidate_classes[cell[0]]['positive_candidate_entities'] = set()\n",
    "                    candidate_classes[cell[0]]['negative_candidate_entities'] = set()\n",
    "                    candidate_classes[cell[0]]['general_positive_entities'] = set()\n",
    "#             print(neighours)\n",
    "            for candidate_class in neighours:\n",
    "                temp = neighours.copy()\n",
    "                temp.remove(candidate_class)\n",
    "#                 print(temp)\n",
    "                candidate_classes[candidate_class]['cooccuring_classes'].update(temp)\n",
    "\n",
    "#             print(file, '--->', col, '--->', neighours)\n",
    "                \n",
    "    return candidate_classes\n",
    "\n",
    "type_neighours_pos_neg_samples = get_candidate_classes(dict_col_candidate_classes)\n",
    "\n",
    "# for key in type_neighours_pos_neg_samples:\n",
    "#     print(f\"Class:{key} with \\t {len(type_neighours_pos_neg_samples[key]['cooccuring_classes'])} neighbouring classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Populate the 'positive_candidate_entities' key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 316/316 [00:01<00:00, 243.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# update the positive samples for each candidate class\n",
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'].update(set(df_entities[df_entities.type == candidate_cls].entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Populate the 'negative_candidate_entities' key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 316/316 [04:01<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# update the negative samples for each class\n",
    "\n",
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    for neighbour_cls in type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes']:\n",
    "        type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'].update(set(df_entities[df_entities.type == neighbour_cls].entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Populate the 'general_positive_entities' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 316/316 [07:06<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    limit = len(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities']=set(get_dbo_class_entities_sparql(candidate_cls,limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 316/316 [00:01<00:00, 221.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'] = list(type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities'])\n",
    "    \n",
    "with open(('type_neighours_pos_neg_samples-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(type_neighours_pos_neg_samples, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the samples from sets to list in order to be able to save the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type_neighours_pos_neg_samples['Fern']\n",
    "len(get_dbo_class_entities_sparql('Fern'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for candidate_cls in type_neighours_pos_neg_samples:\n",
    "#     print(candidate_cls, len(type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\\\n",
    "#           , len(type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)\n",
    "    \n",
    "type_neighours_pos_neg_samples = load_json(cell_values_directory+'type_neighours_pos_neg_samples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooccuring_classes': ['MilitaryPerson',\n",
       "  'Musical',\n",
       "  'Country',\n",
       "  'MusicalArtist',\n",
       "  'Film',\n",
       "  'PopulatedPlace',\n",
       "  'BaseballTeam',\n",
       "  'SportsTeam',\n",
       "  'Town',\n",
       "  'MilitaryConflict',\n",
       "  'SportsEvent',\n",
       "  'Organisation',\n",
       "  'Book',\n",
       "  'NaturalPlace',\n",
       "  'AmericanFootballTeam',\n",
       "  'Ship',\n",
       "  'TelevisionStation',\n",
       "  'Colour',\n",
       "  'BaseballPlayer',\n",
       "  'Athlete',\n",
       "  'PoliticalParty',\n",
       "  'ComicsCharacter',\n",
       "  'Company',\n",
       "  'Artist',\n",
       "  'Group',\n",
       "  'SoccerClub',\n",
       "  'Mountain',\n",
       "  'Drug',\n",
       "  'Region',\n",
       "  'PersonFunction',\n",
       "  'Poem',\n",
       "  'EducationalInstitution',\n",
       "  'Album',\n",
       "  'SportsClub',\n",
       "  'University',\n",
       "  'SocietalEvent',\n",
       "  'Royalty',\n",
       "  'AmusementParkAttraction',\n",
       "  'FictionalCharacter',\n",
       "  'Award',\n",
       "  'TelevisionShow',\n",
       "  'Aircraft',\n",
       "  'HockeyTeam',\n",
       "  'Location',\n",
       "  'Settlement',\n",
       "  'WineRegion',\n",
       "  'Software',\n",
       "  'Band',\n",
       "  'MusicGenre',\n",
       "  'City',\n",
       "  'Place',\n",
       "  'Island',\n",
       "  'MilitaryUnit',\n",
       "  'CricketTeam',\n",
       "  'Person',\n",
       "  'AdministrativeRegion',\n",
       "  'BasketballTeam',\n",
       "  'Song',\n",
       "  'SpaceMission',\n",
       "  'ArchitecturalStructure',\n",
       "  'Event',\n",
       "  'VideoGame',\n",
       "  'MeanOfTransportation',\n",
       "  'Work',\n",
       "  'MusicalWork',\n",
       "  'WrittenWork',\n",
       "  'Agent'],\n",
       " 'positive_candidate_entities': ['Toronto_Argonauts',\n",
       "  'Saskatchewan_Roughriders'],\n",
       " 'negative_candidate_entities': ['Marc_Norman',\n",
       "  'Super_Mario_Galaxy',\n",
       "  'Districts_of_Peru',\n",
       "  'Gustav_Hasford',\n",
       "  'Fugitive',\n",
       "  'Tat_Ming_Pair',\n",
       "  'Chevron_Cars_Ltd',\n",
       "  '2009_swine_flu_pandemic',\n",
       "  'Three_Uses_of_the_Knife',\n",
       "  '500_Days_of_Summer',\n",
       "  'Billy_Bob_Thornton',\n",
       "  'John_C._Reilly',\n",
       "  'Raphael_of_Brooklyn',\n",
       "  'Deus_Ex:_Invisible_War',\n",
       "  'Warcraft_III:_Reign_of_Chaos',\n",
       "  'Kingdom_of_France',\n",
       "  'Metal_Arms:_Glitch_in_the_System',\n",
       "  'Don_Mueang_International_Airport',\n",
       "  'Guatemala_City',\n",
       "  'Big_band',\n",
       "  'Regional_Municipality_of_Wood_Buffalo',\n",
       "  'Street_Fighter_Alpha_2',\n",
       "  'Super_Castlevania_IV',\n",
       "  'Rudy_Giuliani',\n",
       "  'Orson_Welles',\n",
       "  'Judy_Greer',\n",
       "  'Donkey_Kong_Country:_Tropical_Freeze',\n",
       "  'Jerry_Armstrong',\n",
       "  'Blue_Ridge_Reservoir',\n",
       "  'Lake_Lubāns',\n",
       "  'ATV_Offroad_Fury_4',\n",
       "  'Beda_Fell',\n",
       "  'Kiss_of_the_Dragon',\n",
       "  'The_Truman_Show',\n",
       "  'Final_Fantasy_VI',\n",
       "  'The_X_Factor_(British_series_5)',\n",
       "  'Cliff_Mallam',\n",
       "  'United_States',\n",
       "  'Virtua_Fighter_5',\n",
       "  'RollerCoaster_Tycoon_3',\n",
       "  'Big_Show',\n",
       "  'Royal_Bank_of_Scotland',\n",
       "  'Larry_Charles',\n",
       "  'Robert_Pike_(bishop)',\n",
       "  'West_Africa_Time',\n",
       "  'David_Lean_filmography',\n",
       "  'Tales_of_the_Abyss',\n",
       "  'Jean-Luc_Godard',\n",
       "  'Tom_Dowd_(game_designer)',\n",
       "  'Mel_Brooks',\n",
       "  'Mike_Cubbage',\n",
       "  'List_of_King_of_the_Hill_episodes',\n",
       "  'Webbers_Falls_Lake',\n",
       "  'Enter_the_Matrix',\n",
       "  'The_Brady_Bunch',\n",
       "  'William_Friedkin',\n",
       "  'Todd_Helton',\n",
       "  'Bobby_Dodd_Stadium',\n",
       "  'Matthias_Corvinus',\n",
       "  \"Kirby's_Return_to_Dream_Land\",\n",
       "  'Habsburg_Monarchy',\n",
       "  'Nicholas_Webster',\n",
       "  'Death_Race_for_Love',\n",
       "  'King_Kong_(2005_film)',\n",
       "  'Progress_in_Neuro-Psychopharmacology_&_Biological_Psychiatry',\n",
       "  'Alexander_Payne',\n",
       "  'The_100_(TV_series)',\n",
       "  'Goldfinger_(band)',\n",
       "  'Red_River_Campaign',\n",
       "  'Neill_Blomkamp',\n",
       "  'Star_Wars:_Shadows_of_the_Empire',\n",
       "  'Veronica_Maggio',\n",
       "  'Ashmolean_Museum',\n",
       "  'Super_Mario_Kart',\n",
       "  'Tony_Richardson',\n",
       "  'The_Astrophysical_Journal',\n",
       "  'Todd_Crag',\n",
       "  'Screaming_Lord_Sutch_and_the_Savages',\n",
       "  'Costa_Rica_national_football_team',\n",
       "  'Leo_McCarey',\n",
       "  'Transvolga',\n",
       "  'Medical_Journal_of_Australia',\n",
       "  'Mario_Tennis_Aces',\n",
       "  'American_Samoa',\n",
       "  'The_Usual_Suspects',\n",
       "  'Antarctica',\n",
       "  'Crazy_Taxi_3:_High_Roller',\n",
       "  'Matthias_Grünewald',\n",
       "  'Call_of_Duty:_Finest_Hour',\n",
       "  'Lake_Arrowhead,_California',\n",
       "  'Boogie_(genre)',\n",
       "  \"People's_Alliance_for_Democracy\",\n",
       "  'Serbia',\n",
       "  'Eric_Darnell',\n",
       "  'Dawn_of_the_Dead_(2004_film)',\n",
       "  'Quantum_Leap',\n",
       "  \"McDonald's\",\n",
       "  'Ghost_Patrol',\n",
       "  'Stratford-upon-Avon',\n",
       "  'Habits_(Stay_High)',\n",
       "  'Koyaanisqatsi',\n",
       "  'Liam_Lynch_(musician)',\n",
       "  'Planes,_Trains_and_Cars',\n",
       "  'Wario_Land_4',\n",
       "  'Cedar_Creek_Reservoir_(Texas)',\n",
       "  'Red_Dead_Redemption',\n",
       "  'Fountain_County,_Indiana',\n",
       "  'Super_8_(hotel)',\n",
       "  'Kwamina',\n",
       "  'Leslie_H._Martinson',\n",
       "  'Fable_II_Pub_Games',\n",
       "  \"Godspeed_on_the_Devil's_Thunder\",\n",
       "  'Whatì',\n",
       "  'Pope_Clement_IV',\n",
       "  'Svalbard',\n",
       "  'Sonic_Advance',\n",
       "  'U.S._Route_189',\n",
       "  'Newlyweds:_Nick_and_Jessica',\n",
       "  'Perfect_Dark_(series)',\n",
       "  'Royal_College_of_Physicians',\n",
       "  'Nicholas_de_Pencier',\n",
       "  'T._E._Lawrence',\n",
       "  'List_of_The_Golden_Girls_episodes',\n",
       "  'Mary_J._Blige',\n",
       "  'Snakes_on_a_Plane',\n",
       "  'The_Simpsons',\n",
       "  'Tel_Aviv_Museum_of_Art',\n",
       "  'Ninja_Gaiden',\n",
       "  'Shrek_the_Third',\n",
       "  'Le_Mans',\n",
       "  'Arnold_Ross',\n",
       "  'Saint_Anselm_College',\n",
       "  'Steel_Knotts',\n",
       "  '(The_Man_Who_Shot)_Liberty_Valance',\n",
       "  'Grey_Crag',\n",
       "  'Hartford_Whalers',\n",
       "  'Gran_Turismo_(2009_video_game)',\n",
       "  '3_Ninjas',\n",
       "  'Lake_Mendota',\n",
       "  'Barbados_national_football_team',\n",
       "  'Takashi_Shimizu',\n",
       "  'Eppa_Rixey',\n",
       "  'Mahatma_Gandhi',\n",
       "  'CAMEX',\n",
       "  'Guyana',\n",
       "  'Monopoly_video_games',\n",
       "  'Clark_Griffith_Collegiate_Baseball_League',\n",
       "  'RoboCop',\n",
       "  'A_Scanner_Darkly_(film)',\n",
       "  'Dynasty_Warriors_6',\n",
       "  'Molecular_biology',\n",
       "  'Ruble',\n",
       "  'Moscow',\n",
       "  'Beech_Fork_Lake',\n",
       "  'Sheldon_Lake',\n",
       "  'Borderlands_(series)',\n",
       "  'Half-Life_2:_Episode_One',\n",
       "  'Various_(band)',\n",
       "  'The_Idea_Factory',\n",
       "  'The_Amazing_Race_7',\n",
       "  'Taoyuan,_Taiwan',\n",
       "  'Prison_Break_(season_1)',\n",
       "  'Albert_Hughes_(bishop)',\n",
       "  'Stephen_Fincher',\n",
       "  'The_Wages_of_Fear',\n",
       "  'WAMU',\n",
       "  'Deacon_Phillippe',\n",
       "  'Rare_(company)',\n",
       "  'Cyprus',\n",
       "  'Indosiar',\n",
       "  'Spellbound_(2002_film)',\n",
       "  'Honister_Crag_SSSI',\n",
       "  'Liberal_Democrats_(UK)',\n",
       "  'Nip/Tuck',\n",
       "  'Halifax,_Nova_Scotia',\n",
       "  'Cathedral_of_Saint_Elias_and_Saint_Gregory_the_Illuminator',\n",
       "  'Bob_Feller',\n",
       "  'British_NVC_community_M20',\n",
       "  'Good_Will_Hunting',\n",
       "  'Jake_Kasdan',\n",
       "  'Robert_Altman',\n",
       "  'Time_in_Thailand',\n",
       "  'High_Raise_(Langdale)',\n",
       "  'Madden_NFL_09',\n",
       "  'Bad_Arolsen',\n",
       "  'Weisman_Art_Museum',\n",
       "  \"Girls'_Generation\",\n",
       "  'Tony_Fernandez_(musician)',\n",
       "  'Art_museum',\n",
       "  'Kevin_Brownlow',\n",
       "  'Anselm_of_Canterbury',\n",
       "  'Graham_Chapman',\n",
       "  'Blazing_Angels:_Squadrons_of_WWII',\n",
       "  'Little_Rock,_Arkansas',\n",
       "  'Hartsfield–Jackson_Atlanta_International_Airport',\n",
       "  'Dragon_Age:_Origins',\n",
       "  'Laotian_Civil_War',\n",
       "  'Cathedral_of_Our_Lady_(Antwerp)',\n",
       "  'Out_of_the_Past_into_the_Future',\n",
       "  'Rayman',\n",
       "  'Ornithology',\n",
       "  'Red_Lake_shootings',\n",
       "  'Merseburg',\n",
       "  'Medal_of_Honor:_Rising_Sun',\n",
       "  'Boris_Johnson',\n",
       "  'Brown_Bears',\n",
       "  'John_Badham',\n",
       "  'Dale_Township,_Kingman_County,_Kansas',\n",
       "  'Lake_Overholser',\n",
       "  'British_NVC_community_H6',\n",
       "  'India',\n",
       "  'Daddy_Yankee',\n",
       "  'Rangers_F.C.',\n",
       "  'Lawyer',\n",
       "  'David_McKeon',\n",
       "  'Speed_Walking',\n",
       "  'Betause',\n",
       "  'Metroid_Fusion',\n",
       "  'Tactical_shooter',\n",
       "  'Izabela_Czartoryska',\n",
       "  'Annunciation_(El_Greco,_Museo_Thyssen-Bornemisza)',\n",
       "  'Kampala',\n",
       "  'Need_for_Speed:_Most_Wanted_(2005_video_game)',\n",
       "  'Albert_Reynolds',\n",
       "  'Chureh_Nab',\n",
       "  'Progressive_rock',\n",
       "  'London',\n",
       "  'Som_Livre',\n",
       "  \"Sonic's_Ultimate_Genesis_Collection\",\n",
       "  'Andorra',\n",
       "  'Lu_Yen-hsun',\n",
       "  'Nassau,_Bahamas',\n",
       "  'Taka_Michinoku',\n",
       "  'The_Lego_Batman_Movie',\n",
       "  'Tripoli',\n",
       "  'Hospitality',\n",
       "  'Allen_Crags',\n",
       "  'Pittsburgh',\n",
       "  'J._Hugo_Aronson',\n",
       "  'Mark_Kac',\n",
       "  'Dog_Bites_Man',\n",
       "  'Vincent_Gallo',\n",
       "  'Swingers_(2002_film)',\n",
       "  'Old_Trafford',\n",
       "  'Gore_Verbinski',\n",
       "  'Paper_Heart',\n",
       "  'Dunstan',\n",
       "  'George_Stuart_Gordon',\n",
       "  'David_Lean',\n",
       "  'Vyacheslav_Ovchinnikov',\n",
       "  'Tombstone,_Arizona',\n",
       "  'Fireside_Theatre',\n",
       "  'Gears_of_War',\n",
       "  'The_New_York_Times',\n",
       "  'Ernst_Jünger',\n",
       "  'Turok_2:_Seeds_of_Evil_(Game_Boy_Color)',\n",
       "  'Sam_Mendes',\n",
       "  'Dead_Space_(series)',\n",
       "  'French_Air_Force',\n",
       "  'Monthly_Notices_of_the_Royal_Astronomical_Society',\n",
       "  'Robert_Robson',\n",
       "  'Megamind_(video_game)',\n",
       "  \"Yoshi's_Story\",\n",
       "  'Belgium_national_football_team',\n",
       "  'Lake_Granbury',\n",
       "  'Bob_Odenkirk',\n",
       "  'Mosfilm',\n",
       "  'Gut_Records',\n",
       "  'Boulevard_Haussmann',\n",
       "  'Mexico_City_International_Airport',\n",
       "  'St._Vincent_Ferrer_Seminary',\n",
       "  'Top_Hat',\n",
       "  'Action-adventure_game',\n",
       "  'App_Store_(iOS)',\n",
       "  'Frank_Capra_Jr.',\n",
       "  'John_Terry',\n",
       "  'Hercules_(1997_film)',\n",
       "  'The_Infamous_Stringdusters',\n",
       "  'Auchan',\n",
       "  'Alberta',\n",
       "  'Teresa_of_Ávila',\n",
       "  'The_Karate_Kid',\n",
       "  'The_Day_the_Earth_Stood_Still',\n",
       "  'Source_(game_engine)',\n",
       "  'Fred_Zinnemann',\n",
       "  'Amadjuak_Lake',\n",
       "  'La_Dolce_Vita',\n",
       "  'Vertigo_Comics',\n",
       "  'Gossip',\n",
       "  'Uvs_Lake',\n",
       "  'Museum_de_Fundatie',\n",
       "  'James_Gray_(British_politician)',\n",
       "  'Thirlmere',\n",
       "  'Ghost_Lake',\n",
       "  'The_Sea_Inside',\n",
       "  'Jay_Sandrich',\n",
       "  'Scapegoat_Wax',\n",
       "  'RoboCop_(franchise)',\n",
       "  'Drag_queen',\n",
       "  'Dr._Alban',\n",
       "  'Hobart_Muir_Smith',\n",
       "  'Johann_Bernoulli',\n",
       "  'Seattle–Tacoma_International_Airport',\n",
       "  'Newport_County_A.F.C.',\n",
       "  'Batman:_Gotham_Knights',\n",
       "  'Tom_Schuman',\n",
       "  'Anna_Jagiellon,_Duchess_of_Pomerania',\n",
       "  'Music_of_the_Fable_series',\n",
       "  'Carrock_Fell',\n",
       "  'Impact!_(TV_series)',\n",
       "  'The_Legacy_(professional_wrestling)',\n",
       "  'Shaun_of_the_Dead',\n",
       "  'Hoot_Evers',\n",
       "  'Ischia',\n",
       "  'BlackSite:_Area_51',\n",
       "  'Tengai_Makyou:_Ziria',\n",
       "  'Scott_Plank',\n",
       "  'Ronnie_Shelton',\n",
       "  'Sorority_Boys',\n",
       "  'George_McQuinn',\n",
       "  'Physical_Graffiti',\n",
       "  'Martin_McDonagh',\n",
       "  'Wargame_(video_games)',\n",
       "  'Pope_Hyginus',\n",
       "  'Menkemaborg',\n",
       "  'E.T._the_Extra-Terrestrial',\n",
       "  'Armageddon_(Swedish_band)',\n",
       "  'B._Reeves_Eason',\n",
       "  'Govinda_(actor)',\n",
       "  'Geophysical_Journal_International',\n",
       "  'United_States_occupation_of_Nicaragua',\n",
       "  'Sunshine_Cleaning',\n",
       "  'Billy_Ray_Cyrus',\n",
       "  'Lucky_Number_Slevin',\n",
       "  'Speed_Racer',\n",
       "  'Courtauld_Gallery',\n",
       "  'Renkum',\n",
       "  'Joseph_Valentin_Boussinesq',\n",
       "  'Immanuel_Kant',\n",
       "  'Super_Troopers',\n",
       "  'Shizuoka_Prefecture',\n",
       "  'Alejandra_Matus',\n",
       "  'Holy_See',\n",
       "  'The_Muppet_Christmas_Carol',\n",
       "  'Lake_Phalen',\n",
       "  'Artdink',\n",
       "  'Luxembourg',\n",
       "  'The_Dark_Knight_Rises',\n",
       "  'Cayman_Islands_national_football_team',\n",
       "  'Kunstmuseum_Den_Haag',\n",
       "  'Nouakchott',\n",
       "  'Greenhill_mine',\n",
       "  'Sports_game',\n",
       "  'Western_Australia_cricket_team',\n",
       "  'Slap_Shot',\n",
       "  'The_Last_of_the_Mohicans_(1992_film)',\n",
       "  'Strasbourg_Crucifixion',\n",
       "  'Thelma_Connell',\n",
       "  'Egypt',\n",
       "  'Billy_Steinberg',\n",
       "  'Tomb_Raider_(2013_video_game)',\n",
       "  'Yasuharu_Takanashi',\n",
       "  'Guernsey',\n",
       "  'Bartholomew_County,_Indiana',\n",
       "  'Friedrich_Wilhelm_Murnau_Foundation',\n",
       "  'The_Birth_of_a_Nation',\n",
       "  'R._Crumb_&_His_Cheap_Suit_Serenaders',\n",
       "  'Wigfield',\n",
       "  \"Uncharted:_Drake's_Fortune_Original_Soundtrack_from_the_Video_Game\",\n",
       "  'Experimental_music',\n",
       "  'James_II_of_England',\n",
       "  'Museo_Nacional_de_Bellas_Artes_(Buenos_Aires)',\n",
       "  '13_(Blur_album)',\n",
       "  'Dennis_Dugan',\n",
       "  'Heinrich_von_Kittlitz',\n",
       "  'Rest_Dodd',\n",
       "  'Bad_Azz_(rapper)',\n",
       "  'Chicago_Cubs_minor_league_players',\n",
       "  'Oranjestad,_Aruba',\n",
       "  'Pikmin_2',\n",
       "  'Whiteless_Pike',\n",
       "  'Louisa_Holthuysen',\n",
       "  'Scotland',\n",
       "  'Mike_Judge',\n",
       "  'Eraserhead',\n",
       "  'Saw_II',\n",
       "  'Orlando_Predators',\n",
       "  'Battlefield_2',\n",
       "  'Stephen_Daldry',\n",
       "  'The_Legend_of_Zelda:_The_Wind_Waker',\n",
       "  'Chanel',\n",
       "  'Elena_Anaya',\n",
       "  'Peter_Bogdanovich',\n",
       "  'Glendola_Reservoir',\n",
       "  'Flawless_(Beyoncé_song)',\n",
       "  'Pope_Boniface_VIII',\n",
       "  'Whin_Rigg',\n",
       "  'Wandope',\n",
       "  'Brunei',\n",
       "  'Rolex',\n",
       "  'Pretoria_Art_Museum',\n",
       "  'FIFA_Football_2005',\n",
       "  'Jack_Quinn_(politician)',\n",
       "  'Guangzhou_Baiyun_International_Airport_(former)',\n",
       "  'Dominica',\n",
       "  'Grêmio_Esportivo_Brasil',\n",
       "  'The_EMBO_Journal',\n",
       "  'Death-doom',\n",
       "  'Colin_McRae:_Dirt',\n",
       "  'Zack_and_Miri_Make_a_Porno',\n",
       "  'List_of_communities_in_Yukon',\n",
       "  'Colour_Me_Kubrick',\n",
       "  'Trinidad_and_Tobago',\n",
       "  'Juan_Jose_Peruyero',\n",
       "  'All_About_Eve_(band)',\n",
       "  'Otto_Preminger',\n",
       "  'Ed_Wood',\n",
       "  'Bob_Wise',\n",
       "  'Characters_of_Kingdom_Hearts',\n",
       "  'Wave_Race_64',\n",
       "  'Pope_Leo_III',\n",
       "  'Magnum_T.A.',\n",
       "  'Mario_Party_DS',\n",
       "  'Slumdog_Millionaire',\n",
       "  'John_Sturges',\n",
       "  'Dante_Bichette',\n",
       "  'Elizabeth_II',\n",
       "  'Praia',\n",
       "  'Kenny_Rogers_and_The_First_Edition',\n",
       "  'Andre_Thornton',\n",
       "  'Circulation_(journal)',\n",
       "  'Space_rendezvous',\n",
       "  'Doom_metal',\n",
       "  'Madden_NFL_2003',\n",
       "  'Bar/None_Records',\n",
       "  'Heavenly_Creatures',\n",
       "  'Final_Fantasy_VIII',\n",
       "  'Morocco',\n",
       "  'Mary_Tyler_Moore',\n",
       "  'Saint_Kitts_and_Nevis',\n",
       "  'Fallout_(series)',\n",
       "  'Iván_DeJesús',\n",
       "  'Sucre_(state)',\n",
       "  'Eastern_plantain-eater',\n",
       "  'Lake_Anna_Park',\n",
       "  'American_Gangster_(film)',\n",
       "  'Roger_Hargreaves',\n",
       "  'George_Uhle',\n",
       "  'Planet_Terror',\n",
       "  'The_Master_(2012_film)',\n",
       "  'Kina_(musician)',\n",
       "  'Beloye_Ozero',\n",
       "  'The_Bourne_Supremacy',\n",
       "  'Inter_IKEA_Holding',\n",
       "  'Wright_Patman_Lake',\n",
       "  \"Donkey_Kong_Country_2:_Diddy's_Kong_Quest\",\n",
       "  'Curse_of_the_Golden_Flower_(EP)',\n",
       "  'Lake_of_the_Woods_County,_Minnesota',\n",
       "  'Is_It_Fall_Yet%3F',\n",
       "  'Bavarian_Soviet_Republic',\n",
       "  'Orlando_Pirates_F.C.',\n",
       "  \"The_Sims_Bustin'_Out\",\n",
       "  'J._Paul_Getty_Museum',\n",
       "  'The_Exorcist_(film)',\n",
       "  'Grand_Prix_motorcycle_racing',\n",
       "  'Jack_Black',\n",
       "  'Drama',\n",
       "  'Bill_Condon_(footballer)',\n",
       "  'Los_Angeles_Dodgers',\n",
       "  'Electronic_Arts',\n",
       "  'Robert_Mark_Kamen',\n",
       "  'Andrei_Rublev_(film)',\n",
       "  'Phoenix,_Arizona',\n",
       "  'Sleeping_with_Charlie_Kaufman',\n",
       "  'Scientific_American',\n",
       "  'Andrei_Tarkovsky',\n",
       "  'Journal_of_Virology',\n",
       "  'Freetown,_Massachusetts',\n",
       "  'The_Snow_Goose_(album)',\n",
       "  'Green_salamander',\n",
       "  'Clone_Wars_(Star_Wars)',\n",
       "  'Naira,_Srikakulam_district',\n",
       "  'Morgan_J._Freeman',\n",
       "  'Top_Gear_(series_3)',\n",
       "  'Bullfrog_Productions',\n",
       "  'Philadelphia_Athletics_(1890–91)',\n",
       "  'Year_of_the_Dog..._Again',\n",
       "  'George_W._Bush',\n",
       "  'A_Trip_to_the_Moon',\n",
       "  'Israel',\n",
       "  'Arc_Dream_Publishing',\n",
       "  'Seth_Gordon',\n",
       "  'Pope_Sixtus_III',\n",
       "  'Le_Mans_FC',\n",
       "  'Hedwig_of_Silesia',\n",
       "  'Babe_Ruth',\n",
       "  'Irvin_Kershner',\n",
       "  'Tim_Keefe',\n",
       "  'Hércules_CF',\n",
       "  'Afghanistan',\n",
       "  'Stafford_Sands',\n",
       "  'Jay_Karas',\n",
       "  'Per_Olof_Christopher_Aurivillius',\n",
       "  'Museo_Nacional_de_Artes_Decorativas',\n",
       "  'Sherlock_Jr.',\n",
       "  'Canadian_five-dollar_note',\n",
       "  'Mission:_Impossible_III',\n",
       "  'Marco_Beltrami',\n",
       "  'Gregory_Middleton',\n",
       "  'Worcester_Art_Museum',\n",
       "  'Naruto:_Clash_of_Ninja',\n",
       "  'Royal_Institute_for_Cultural_Heritage',\n",
       "  'Battle_of_Guam_(1944)',\n",
       "  'David_Malloy',\n",
       "  'Namida_Kirari_Tobase',\n",
       "  'Royal_Museum_of_Fine_Arts_Antwerp',\n",
       "  'Mabuwaya',\n",
       "  'United_States_Navy_SEALs',\n",
       "  'Switzerland',\n",
       "  'James_E._Gunn_(writer)',\n",
       "  'Scott_Cooper_(director)',\n",
       "  'Ontario',\n",
       "  'Martin_of_Tours',\n",
       "  'Rodrigo_Santoro',\n",
       "  'Nike_Hoop_Summit',\n",
       "  'Angra_(band)',\n",
       "  'Malcolm_X',\n",
       "  'Fight_Night_2004',\n",
       "  'Lake_End,_Louisiana',\n",
       "  'The_Remains_of_the_Day',\n",
       "  'Frederick_Dunlap_(American_football)',\n",
       "  'Star_Wars:_Episode_I_–_The_Phantom_Menace',\n",
       "  'Jonathan_Daniels',\n",
       "  'Earth,_Wind_&_Fire',\n",
       "  'DGUSA_Enter_the_Dragon',\n",
       "  'Dell_Publishing',\n",
       "  'T2_Trainspotting',\n",
       "  'Mallrats',\n",
       "  'Weaver_Popcorn_Company',\n",
       "  'Joe_Cronin',\n",
       "  'Skiddaw_Slate',\n",
       "  'Sam_Axe',\n",
       "  'Quebec_City',\n",
       "  'Goemon_(series)',\n",
       "  'The_Sims_2',\n",
       "  'Stapleton_International_Airport',\n",
       "  'Ghoul_Patrol',\n",
       "  'MotorStorm_(video_game)',\n",
       "  'Trinidad',\n",
       "  'Charlie_Parker',\n",
       "  'Fantastic_Four:_Rise_of_the_Silver_Surfer',\n",
       "  'Clyde_Bruckman',\n",
       "  'Prince_William_V_Gallery',\n",
       "  'Shopgirl',\n",
       "  'NBA_2K3',\n",
       "  'NFL_Head_Coach_09',\n",
       "  'Alvarado_Park_Lake',\n",
       "  'George_Nolfi',\n",
       "  'Heavy_Metal_(magazine)',\n",
       "  'Citizen_Kane',\n",
       "  'Loughrigg_Fell',\n",
       "  'Tomb_Raider:_Anniversary',\n",
       "  'New_Guinea_campaign',\n",
       "  'Pope_Damasus_II',\n",
       "  'Greek_Revival_architecture',\n",
       "  'Iceland',\n",
       "  'BHP',\n",
       "  'Gran_Turismo_3:_A-Spec',\n",
       "  'Zoolander',\n",
       "  'Comoros',\n",
       "  'Reggie_Jackson',\n",
       "  \"It's_Punky_Brewster\",\n",
       "  '1950s_quiz_show_scandals',\n",
       "  'Romeo_and_Juliet',\n",
       "  'Great_Borne',\n",
       "  'Burlington,_Vermont',\n",
       "  'BioShock',\n",
       "  'Peter_Gabriel',\n",
       "  'The_Adjustment_Bureau',\n",
       "  'Linux_Libertine',\n",
       "  'Donnie_Darko',\n",
       "  'The_Valley,_Anguilla',\n",
       "  'Results_(album)',\n",
       "  'Gregg_Araki',\n",
       "  'The_Queen_and_I_(song)',\n",
       "  'Civilization_IV:_Colonization',\n",
       "  'Mordechai_Yosef_Leiner',\n",
       "  'The_Wheel_of_Time',\n",
       "  'Luis_Tiant_Sr.',\n",
       "  'Intel',\n",
       "  'Granville_Lake',\n",
       "  \"I'm_in_the_Mood_for_Love\",\n",
       "  'Lake_Snowden',\n",
       "  'List_of_The_Brady_Bunch_characters',\n",
       "  'Gareth_Edwards',\n",
       "  'North_African_campaign',\n",
       "  'Delaware',\n",
       "  'Lempira_Department',\n",
       "  'Istanbul',\n",
       "  'List_of_Desperate_Housewives_characters',\n",
       "  'Fallout:_New_Vegas',\n",
       "  'Willy_Brandt',\n",
       "  'Rick_Rude',\n",
       "  'Casino_Royale_(2006_film)',\n",
       "  'Rich_Gedman',\n",
       "  'Oribe_Peralta',\n",
       "  'Napoleon_Dynamite_(TV_series)',\n",
       "  'Madonna_Standing_(van_der_Weyden)',\n",
       "  '1917_(2019_film)',\n",
       "  'Sailor_Moon',\n",
       "  'Croatia_national_football_team',\n",
       "  'Anti-Racist_Action',\n",
       "  'Timur_Bekmambetov',\n",
       "  'Iván_Ramis',\n",
       "  'Disney_Channel',\n",
       "  'Lloyd_Bacon',\n",
       "  'Pope_Cornelius',\n",
       "  'Northern_Cyprus',\n",
       "  'Ennio_Morricone',\n",
       "  \"Tomoyo_After:_It's_a_Wonderful_Life\",\n",
       "  'Dildo_Key',\n",
       "  'Fraggle_Rock',\n",
       "  'V_for_Vendetta_(film)',\n",
       "  'David_O._Russell',\n",
       "  'D._W._Griffith_filmography',\n",
       "  'Pittsburgh_Steelers',\n",
       "  'A&M_Records',\n",
       "  'George_Uhlenbeck',\n",
       "  'Casino_Royale_(1967_film)',\n",
       "  'RollerCoaster_Tycoon_2',\n",
       "  'Annie_Hall',\n",
       "  'The_Home_Depot',\n",
       "  'Southeast_Slovenia_Statistical_Region',\n",
       "  'Call_of_Duty:_Black_Ops_II',\n",
       "  'Jon_Favreau_(speechwriter)',\n",
       "  'Iowa',\n",
       "  'Halo:_Reach',\n",
       "  'Diamond_Jim',\n",
       "  'Western_Sahara',\n",
       "  'Eden_Eternal',\n",
       "  'Zach_Helm',\n",
       "  'IKEA',\n",
       "  'Final_Fantasy_Adventure',\n",
       "  'New_Girl_(season_2)',\n",
       "  'Mongolia',\n",
       "  'Deliverance_(metal_band)',\n",
       "  'North_Dakota',\n",
       "  'Lake_Rosseau',\n",
       "  'Wadsworth_Atheneum',\n",
       "  'Garry_Marshall',\n",
       "  'The_Legend_of_Zelda:_Phantom_Hourglass',\n",
       "  'Ramakrishna',\n",
       "  'Gran_Turismo_5_Prologue',\n",
       "  'Charmed_(season_8)',\n",
       "  'The_Imaginarium_of_Doctor_Parnassus',\n",
       "  'Wake_the_Dead',\n",
       "  'Robert_Alan_Aurthur',\n",
       "  'J._Proctor_Knott',\n",
       "  'Super_Adventure_Island',\n",
       "  'Need_for_Speed:_Shift',\n",
       "  'Ron_Howard',\n",
       "  '50_Cent:_Blood_on_the_Sand',\n",
       "  'Tamsoft',\n",
       "  'The_Bridge_over_the_River_Kwai',\n",
       "  'Gregory_Hoblit',\n",
       "  'Fantastic_Voyage',\n",
       "  'HLA_(journal)',\n",
       "  'Pilot_Knob_State_Park',\n",
       "  'David_Kopp',\n",
       "  'Fast_Times_at_Ridgemont_High',\n",
       "  'No_Strings_Attached_(NSYNC_album)',\n",
       "  \"Luigi's_Mansion:_Dark_Moon\",\n",
       "  'Calling_All_Cars_(The_Sopranos)',\n",
       "  'Incheon-class_frigate',\n",
       "  'One_Piece',\n",
       "  'Minority_Report_(film)',\n",
       "  'Grand_Theft_Auto:_San_Andreas',\n",
       "  'The_Beast_of_Babylon_Against_the_Son_of_Hercules',\n",
       "  'Jackson,_Michigan',\n",
       "  'Lake_Oconee',\n",
       "  'Young_People_Fucking',\n",
       "  'The_Hot_Chick',\n",
       "  'Steven_Lisberger',\n",
       "  'Norway',\n",
       "  'Cloverfield',\n",
       "  '1927_Far_Eastern_Championship_Games',\n",
       "  'Total_S.A.',\n",
       "  'Gabriel_Yared',\n",
       "  'Nap_Lajoie',\n",
       "  'U.S._Route_12_in_Wisconsin',\n",
       "  'Chemnitzer_FC',\n",
       "  'Elliot_Aronson',\n",
       "  'Jonathan_Larson',\n",
       "  'Henk_Rogers',\n",
       "  'School_District_23_Central_Okanagan',\n",
       "  'Kenji_Mizoguchi',\n",
       "  'Blade_II',\n",
       "  'China_Airlines',\n",
       "  'Just_a_Song_Before_I_Go',\n",
       "  'Day_of_the_Tentacle',\n",
       "  'Kolchak:_The_Night_Stalker',\n",
       "  'Directors_Label',\n",
       "  'Akai_Katana',\n",
       "  'Cy_Young_Award',\n",
       "  \"Breakfast_at_Tiffany's_(novella)\",\n",
       "  'The_Dark_Knight_(film)',\n",
       "  'Heinrich_von_Kleist',\n",
       "  'Simon_Birch',\n",
       "  'Tom_Jones_(singer)',\n",
       "  'The_Bottle_Rockets',\n",
       "  'High_Museum_of_Art',\n",
       "  'Babe_Herman',\n",
       "  'Richard_Marsland',\n",
       "  'Charles_Laughton',\n",
       "  'Issyk-Kul_Region',\n",
       "  'Shakira',\n",
       "  'Eddie_Murray_(American_football)',\n",
       "  'Claude_R._Lakey',\n",
       "  'Brazil',\n",
       "  'Bonny_Lake_(Florida)',\n",
       "  'Dorking',\n",
       "  'Richard_Shepard',\n",
       "  'Action_game',\n",
       "  'Rolex_Sports_Car_Series',\n",
       "  'Julie_Andrews',\n",
       "  'Kunsthalle_Hamburg',\n",
       "  'Egyptian_goose',\n",
       "  'Lake_Geneva,_Wisconsin',\n",
       "  'Bond_Girls_Are_Forever',\n",
       "  'Sanabal_Charitable_Committee',\n",
       "  'Museo_Poldi_Pezzoli',\n",
       "  'Poland',\n",
       "  'Victory_by_Design',\n",
       "  'Turkey_national_under-21_football_team',\n",
       "  'Ivan_Reitman',\n",
       "  'Zoo_Tycoon_2',\n",
       "  'Toronto_Blue_Jays',\n",
       "  'Mike_Judge_Presents:_Tales_from_the_Tour_Bus',\n",
       "  'Incheon_International_Airport',\n",
       "  'Buffalo,_New_York',\n",
       "  'Eritrea',\n",
       "  'Jeremy_Leven',\n",
       "  'The_Lives_of_Others_(novel)',\n",
       "  'Caveh_Zahedi',\n",
       "  'Birdman_(rapper)',\n",
       "  'Museo_Nacional_de_Escultura,_Valladolid',\n",
       "  'UFC_2009_Undisputed',\n",
       "  'Jeff_Bleckner',\n",
       "  'North_24_Parganas_district',\n",
       "  'Edmonton',\n",
       "  'Island_Records',\n",
       "  'Northern_Territory',\n",
       "  'Stefanie_Melbeck',\n",
       "  'J._C._P._Williams',\n",
       "  'École_Belge_de_Bujumbura',\n",
       "  'Sistar',\n",
       "  'Armored_Core_V',\n",
       "  'Austria-Hungary',\n",
       "  'Bill_Nolan_(animator)',\n",
       "  'Titanic_(1997_film)',\n",
       "  'Minnesota',\n",
       "  'Milwaukee_Brewers',\n",
       "  'Broken_Sword',\n",
       "  'List_of_awards_and_nominations_received_by_Sylvester_Stallone',\n",
       "  'Broken_Flowers',\n",
       "  'Bermuda',\n",
       "  'George_McLean_(footballer,_born_1943)',\n",
       "  'Cornell_Law_School',\n",
       "  'Georg_Cantor_Gymnasium',\n",
       "  'Bill_Bradley_(American_football)',\n",
       "  'Cries_and_Whispers',\n",
       "  'Oberer_Murgsee',\n",
       "  'Juneau,_Alaska',\n",
       "  'CDV_Software',\n",
       "  'The_Empire_Strikes_Back',\n",
       "  'Tales_of_Dunk_and_Egg',\n",
       "  'James_the_Less',\n",
       "  'The_Green_Mile_(novel)',\n",
       "  'Red_Road_Flats',\n",
       "  'John_Mayberry_Jr.',\n",
       "  'Kronan_(ship)',\n",
       "  'Shanghai_Greenland_Shenhua_F.C.',\n",
       "  'The_Secret_of_Monkey_Island',\n",
       "  'Paul_(bishop_of_Mérida)',\n",
       "  'Stevie_Wonder',\n",
       "  'A_Clockwork_Orange',\n",
       "  'Midnight_Club_II',\n",
       "  'Ray_Lankford',\n",
       "  'Children_of_Men',\n",
       "  'Gucci_Mane_discography',\n",
       "  'School_District_57_Prince_George',\n",
       "  'Lake_Zurich',\n",
       "  'Ashy-crowned_sparrow-lark',\n",
       "  'Easedale_Tarn',\n",
       "  'Nick_Cave_and_the_Bad_Seeds',\n",
       "  'New_Brunswick',\n",
       "  'Nosferatu_D2',\n",
       "  'Dead_or_Alive_2:_Birds',\n",
       "  'Deuce_(wrestler)',\n",
       "  'United_States_one-dollar_bill',\n",
       "  'Saint_Timothy',\n",
       "  'Club_Bolívar',\n",
       "  'John_Bunch_Plays_Kurt_Weill',\n",
       "  'Red_Lake_Falls,_Minnesota',\n",
       "  'Tremaine_the_Album',\n",
       "  'J._Cole',\n",
       "  'Mike_Blowers',\n",
       "  'Saints_Cyril_and_Methodius',\n",
       "  'The_Hurt_Locker',\n",
       "  'Chaos;Head',\n",
       "  'Gary_Varsho',\n",
       "  'Brooklyn_Museum_Art_School',\n",
       "  'Kinshasa',\n",
       "  'Fahrenheit_451',\n",
       "  'Punjab_Kesari_(film)',\n",
       "  'Norman_Jewson',\n",
       "  'Caw_Fell',\n",
       "  'Nancy_Drew:_Ghost_Dogs_of_Moon_Lake',\n",
       "  'David_Yates_(legal_scholar)',\n",
       "  'SNK',\n",
       "  'Semi-Pro',\n",
       "  'Armik',\n",
       "  'Micronesia',\n",
       "  'The_Benchwarmers',\n",
       "  'Andrew_Jackson',\n",
       "  'Liechtenstein',\n",
       "  'New_York_Mets',\n",
       "  'Harry_Potter_and_the_Deathly_Hallows',\n",
       "  'Lake_Limestone',\n",
       "  'Harley_Race',\n",
       "  'Lilongwe_International_Airport',\n",
       "  'Spyro:_Year_of_the_Dragon',\n",
       "  'Oklahoma_State_Highway_74',\n",
       "  'Atlanta_Braves',\n",
       "  'Black-necked_weaver',\n",
       "  'List_of_Monster_Jam_video_games',\n",
       "  'Greg_Maddux',\n",
       "  'Marvel_vs._Capcom_3:_Fate_of_Two_Worlds',\n",
       "  'Julius_and_Ethel_Rosenberg',\n",
       "  'Larry_David',\n",
       "  'Heaven_Help_Us',\n",
       "  'Metropolitan_Museum_of_Art',\n",
       "  'The_Balham_Alligators',\n",
       "  'Fullerton_School_District',\n",
       "  'Stade_Olympique_(Nouakchott)',\n",
       "  'Pope_Adrian_III',\n",
       "  'Kunstmuseum_Liechtenstein',\n",
       "  'Throne_of_Blood',\n",
       "  'Reginald_Rose',\n",
       "  'In_the_Valley_of_Elah',\n",
       "  'Dave_Kingman',\n",
       "  'Lake_Isabella,_California',\n",
       "  'Pro_Evolution_Soccer_2011',\n",
       "  'Nevada',\n",
       "  'La_Femme_Nikita:_Music_from_the_Television_Series',\n",
       "  'Black_sparrowhawk',\n",
       "  'Great_Expectations',\n",
       "  'The_Elder_Scrolls_V:_Skyrim',\n",
       "  'Paulinus_of_Nola',\n",
       "  'Craig_Gillespie',\n",
       "  'Fight_Night_Round_4',\n",
       "  'Animal_House',\n",
       "  '2016_United_States_House_of_Representatives_elections',\n",
       "  'Noah_Wyle',\n",
       "  'Rheinisches_Landesmuseum_Trier',\n",
       "  'Christopher_Bowman',\n",
       "  'Giovanni_Lavaggi',\n",
       "  'Michael_Schultz',\n",
       "  'North_Carolina_Central_University',\n",
       "  'X-Men',\n",
       "  'Lake_Taylorville',\n",
       "  'Belfast_Cromac_(Northern_Ireland_Parliament_constituency)',\n",
       "  'Guitar_Hero_III:_Legends_of_Rock',\n",
       "  'Earl_Warren',\n",
       "  'The_Chronicles_of_Riddick:_Assault_on_Dark_Athena',\n",
       "  'Saint_Lucia–United_States_relations',\n",
       "  'Lake_Pueblo_State_Park',\n",
       "  'Ben-Hur:_A_Tale_of_the_Christ',\n",
       "  'Staatliche_Kunstsammlungen_Dresden',\n",
       "  'Florian_Henckel_von_Donnersmarck',\n",
       "  'Luis_Ángel_González_Macchi',\n",
       "  \"Dr._Horrible's_Sing-Along_Blog_(soundtrack)\",\n",
       "  'Alternative_rock',\n",
       "  'Puerto_Rico',\n",
       "  'Derby_Museum_and_Art_Gallery',\n",
       "  'Governorate_of_Estonia',\n",
       "  'Spirited_Away',\n",
       "  'Count_Your_Blessings_(Bring_Me_the_Horizon_album)',\n",
       "  'Married..._with_Children',\n",
       "  'Charley_Boorman',\n",
       "  'Lester_Ricard',\n",
       "  'Midnight_Club:_Los_Angeles',\n",
       "  'Pope_Pius_X',\n",
       "  'Hebrew_University_of_Jerusalem',\n",
       "  'Thom_Yorke',\n",
       "  'Wes_Watkins_Reservoir',\n",
       "  'Buck_Mountain_(Montana)',\n",
       "  'Captain_Planet_and_the_Planeteers',\n",
       "  'Bhutan_Time',\n",
       "  'Cal_Ripken_Jr.',\n",
       "  'Blazing_Angels_2:_Secret_Missions_of_WWII',\n",
       "  'Abyssinian_Creole',\n",
       "  'Sir_George_Burns,_1st_Baronet',\n",
       "  'Philomena',\n",
       "  'Joe_McGinniss',\n",
       "  'Bamboozled',\n",
       "  'Thailand',\n",
       "  'Tropico_3',\n",
       "  'Impressions_Games',\n",
       "  'Mega_Man_ZX',\n",
       "  'WWF_War_Zone',\n",
       "  'The_Golden_Girls',\n",
       "  'University_of_Basel',\n",
       "  'Hainan_eastern_ring_high-speed_railway',\n",
       "  'Nederlands_Scheepvaartmuseum',\n",
       "  'Bolt_action',\n",
       "  'Fate/Zero',\n",
       "  'Test_Drive_Unlimited',\n",
       "  'Sonic_Adventure_2',\n",
       "  'THQ_Nordic',\n",
       "  'Shawnigan_Lake,_British_Columbia',\n",
       "  'Mink_Stole',\n",
       "  'Super_Adventure_Island_II',\n",
       "  'Machinist',\n",
       "  'Cook_Islands',\n",
       "  'Brian_Henton',\n",
       "  'Gustavo_Bell',\n",
       "  'Victor_Fleming_(basketball)',\n",
       "  'Williams_Lake,_British_Columbia',\n",
       "  'Pomona_State_Park',\n",
       "  'Woody_Allen',\n",
       "  \"Bucket_&_Skinner's_Epic_Adventures\",\n",
       "  'Marine_Corps_Air_Station_Eagle_Mountain_Lake',\n",
       "  'Croc:_Legend_of_the_Gobbos',\n",
       "  'Charlie_Gehringer',\n",
       "  'Puzzle_video_game',\n",
       "  'Nancy_Dowd',\n",
       "  'Steven_Soderbergh',\n",
       "  'Spellbound_Entertainment',\n",
       "  'Missouri',\n",
       "  'Punky_Brewster',\n",
       "  'Ghana_national_football_team',\n",
       "  'Dennis_Eckersley',\n",
       "  'Angel_(band)',\n",
       "  'The_Story_of_Civilization',\n",
       "  'Pirate_radio',\n",
       "  'Army_of_Two:_The_40th_Day',\n",
       "  'One_Hour_Photo',\n",
       "  'Montserrat_cricket_team',\n",
       "  'Osvaldo_Vieira_International_Airport',\n",
       "  'Full_Frontal_(Australian_TV_series)',\n",
       "  'Strategic_Grill_Locations',\n",
       "  'Iron_Man_(2008_film)',\n",
       "  'Aero_the_Acro-Bat',\n",
       "  'Fort_Smith,_Arkansas',\n",
       "  'The_Zany_Adventures_of_Robin_Hood',\n",
       "  'Shortbus',\n",
       "  '2012_Republican_Party_presidential_primaries',\n",
       "  'King_Kendall',\n",
       "  'Rapala_Fishing_Frenzy_2009',\n",
       "  'Nab_Scar',\n",
       "  'Stuntman:_Ignition',\n",
       "  'Batman_v_Superman:_Dawn_of_Justice',\n",
       "  'American_Hockey_League',\n",
       "  'Saint_Kitts_and_Nevis_national_football_team',\n",
       "  'Andrew_Stanton',\n",
       "  'Giulietta_Masina',\n",
       "  'Athanasius_of_Alexandria',\n",
       "  'Total_Chalaco',\n",
       "  'Adalbert_of_Magdeburg',\n",
       "  'David_Anspaugh',\n",
       "  'Eau_Claire_Area_School_District',\n",
       "  'Animal_latrine',\n",
       "  '3_Ninjas_(film)',\n",
       "  'Mr._Smith_Goes_to_Washington',\n",
       "  'Rozbijemy_zabawę',\n",
       "  'Ugetsu_Monogatari',\n",
       "  'Pineapple_Express_(film)',\n",
       "  'Call_of_Duty:_Modern_Warfare_3',\n",
       "  'Robert_Parker_(wine_critic)',\n",
       "  'Benjamin_Brandreth',\n",
       "  'John_Blumenthal',\n",
       "  'Surxondaryo_Region',\n",
       "  'Robert_Wise',\n",
       "  'Allison_Anders',\n",
       "  'FIFA_(video_game_series)',\n",
       "  'Bangladesh_Standard_Time',\n",
       "  'Least_weasel',\n",
       "  'Falklands_War',\n",
       "  'Eyre_&_Spottiswoode',\n",
       "  'Lego_Star_Wars_III:_The_Clone_Wars',\n",
       "  'Cerulean_Warbler_Bird_Reserve',\n",
       "  'The_Cloverfield_Paradox',\n",
       "  'Tetris',\n",
       "  'Gee_Walker',\n",
       "  'Iván_DeJesús_Jr.',\n",
       "  'V_for_Vendetta',\n",
       "  'Senegal_national_football_team',\n",
       "  'Common_land',\n",
       "  'Kingdom_Hearts_II',\n",
       "  ...],\n",
       " 'general_positive_entities': []}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_neighours_pos_neg_samples['CanadianFootballTeam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/city-ds-final-project/SemAIDA-master/AAAI19/exp_T2D/in_out/w2v_model/enwiki_model'),\n",
    "    # default='~/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONLY LOAD ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.text.en import tokenize\n",
    "\n",
    "def generate_synthetic_columns(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            unit.append(ent)\n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = ent.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_samples(pos, neg, pct = 0.5):\n",
    "    if len(pos) <= len(neg):\n",
    "        return pos+[random.choice(pos) for _ in range(math.ceil((len(neg)-len(pos))*pct))], neg\n",
    "    else:\n",
    "        return pos, neg+[random.choice(neg) for _ in range(math.ceil((len(pos)-len(neg))*pct))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(entities_positive, entities_negative):\n",
    "    # embedding\n",
    "    units_positive = generate_synthetic_columns(entities_positive, FLAGS.synthetic_column_size)\n",
    "    units_negative = generate_synthetic_columns(entities_negative, FLAGS.synthetic_column_size)\n",
    "\n",
    "    sequences_positive = list()\n",
    "    for ent_unit in units_positive:\n",
    "        sequences_positive.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "    sequences_negative = list()\n",
    "    for ent_unit in units_negative:\n",
    "        sequences_negative.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "\n",
    "    x = np.zeros((len(sequences_positive) + len(sequences_negative), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for sample_i, sequence in enumerate(sequences_positive + sequences_negative):\n",
    "        x[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    y_positive = np.ones((len(sequences_positive), 1))\n",
    "    y_negative = np.zeros((len(sequences_negative), 1))\n",
    "    y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "    # shuffling\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    return x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(os.getcwd()+'/cnn_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the directory of cnn trained models so that the solution can pick up from where it left off in case there are more classifiers to be trained based on the candidate classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Actor',\n",
       " 'AdministrativeRegion',\n",
       " 'Agent',\n",
       " 'Aircraft',\n",
       " 'Airport',\n",
       " 'Album',\n",
       " 'Animal',\n",
       " 'ArchitecturalStructure',\n",
       " 'Artist',\n",
       " 'Artwork',\n",
       " 'Athlete',\n",
       " 'Award',\n",
       " 'Band',\n",
       " 'Bank',\n",
       " 'BasketballLeague',\n",
       " 'BasketballPlayer',\n",
       " 'BasketballTeam',\n",
       " 'BodyOfWater',\n",
       " 'Book',\n",
       " 'Bridge',\n",
       " 'Broadcaster',\n",
       " 'Building',\n",
       " 'Canal',\n",
       " 'ChemicalCompound',\n",
       " 'ChemicalSubstance',\n",
       " 'City',\n",
       " 'CityDistrict',\n",
       " 'Cleric',\n",
       " 'ClericalAdministrativeRegion',\n",
       " 'ComedyGroup',\n",
       " 'Comic',\n",
       " 'Company',\n",
       " 'Continent',\n",
       " 'Country',\n",
       " 'CricketTeam',\n",
       " 'Criminal',\n",
       " 'Crustacean',\n",
       " 'Dam',\n",
       " 'Device',\n",
       " 'Diocese',\n",
       " 'EducationalInstitution',\n",
       " 'EthnicGroup',\n",
       " 'Eukaryote',\n",
       " 'Event',\n",
       " 'FictionalCharacter',\n",
       " 'Film',\n",
       " 'Fish',\n",
       " 'Food',\n",
       " 'FormulaOneTeam',\n",
       " 'Genre',\n",
       " 'Group',\n",
       " 'HockeyTeam',\n",
       " 'Holiday',\n",
       " 'InformationAppliance',\n",
       " 'Infrastructure',\n",
       " 'Insect',\n",
       " 'Lake',\n",
       " 'Language',\n",
       " 'Location',\n",
       " 'Locomotive',\n",
       " 'Magazine',\n",
       " 'Manga',\n",
       " 'MeanOfTransportation',\n",
       " 'MemberOfParliament',\n",
       " 'MilitaryConflict',\n",
       " 'MilitaryPerson',\n",
       " 'MilitaryUnit',\n",
       " 'Mineral',\n",
       " 'Mountain',\n",
       " 'Museum',\n",
       " 'MusicGenre',\n",
       " 'Musical',\n",
       " 'MusicalArtist',\n",
       " 'MusicalWork',\n",
       " 'NaturalPlace',\n",
       " 'Organisation',\n",
       " 'Person',\n",
       " 'PersonFunction',\n",
       " 'Place',\n",
       " 'Play',\n",
       " 'Politician',\n",
       " 'PopulatedPlace',\n",
       " 'Presenter',\n",
       " 'Prison',\n",
       " 'ProtectedArea',\n",
       " 'PublicTransitSystem',\n",
       " 'RadioProgram',\n",
       " 'RadioStation',\n",
       " 'RecordLabel',\n",
       " 'Region',\n",
       " 'River',\n",
       " 'Road',\n",
       " 'RouteOfTransportation',\n",
       " 'Royalty',\n",
       " 'Saint',\n",
       " 'School',\n",
       " 'Scientist',\n",
       " 'Settlement',\n",
       " 'Ship',\n",
       " 'Single',\n",
       " 'SoccerClub',\n",
       " 'SoccerPlayer',\n",
       " 'SocietalEvent',\n",
       " 'Software',\n",
       " 'Song',\n",
       " 'Species',\n",
       " 'SportsClub',\n",
       " 'SportsEvent',\n",
       " 'SportsLeague',\n",
       " 'SportsTeam',\n",
       " 'Station',\n",
       " 'Stream',\n",
       " 'TelevisionSeason',\n",
       " 'TelevisionShow',\n",
       " 'TelevisionStation',\n",
       " 'TimeInterval',\n",
       " 'TopicalConcept',\n",
       " 'Town',\n",
       " 'University',\n",
       " 'Venue',\n",
       " 'VideoGame',\n",
       " 'Village',\n",
       " 'Work',\n",
       " 'Writer',\n",
       " 'WrittenWork'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 316/316 [2:28:19<00:00, 28.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "\n",
    "batch_size = 32 \n",
    "epochs = 1\n",
    "test_train_split = 0.2\n",
    "\n",
    "def save_model(model, candidate_class):\n",
    "    cwd = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "    model.save(cwd+'/%s' % candidate_class)\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')\n",
    "\n",
    "for candidate_class in tqdm(type_neighours_pos_neg_samples):\n",
    "    if candidate_class not in loaded_models:\n",
    "        print(candidate_class)\n",
    "        # Get the positive and negative samples to train the model\n",
    "        cls_neg_par_entities = list(type_neighours_pos_neg_samples[candidate_class]['negative_candidate_entities'])\n",
    "        cls_pos_gen_entities = list(type_neighours_pos_neg_samples[candidate_class]['general_positive_entities'])\n",
    "\n",
    "        # align the samples to create a balance set\n",
    "        p_ents, n_ents = align_samples(cls_pos_gen_entities, cls_neg_par_entities,1)\n",
    "\n",
    "        # Create the embeddings using the w2v_model. here the samples are shuffled so we have a mixture of positive and negative samples\n",
    "        X, Y = embedding(p_ents, n_ents)\n",
    "\n",
    "        dev_sample_index = int(test_train_split * float(X.shape[0]))\n",
    "        X_train, X_dev = X[dev_sample_index:], X[:dev_sample_index]\n",
    "        Y_train, Y_dev = Y[dev_sample_index:], Y[:dev_sample_index]\n",
    "\n",
    "        IMG_HEIGHT = X_train.shape[1]\n",
    "        IMG_WIDTH = X_train.shape[2]\n",
    "\n",
    "        #Build the model\n",
    "        model = Sequential([\n",
    "            Conv2D(16, 3, padding='same', activation='relu', \n",
    "                   input_shape=(IMG_HEIGHT, IMG_WIDTH ,1)),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        # print the model architecture\n",
    "    #     model.summary()\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                           batch_size=batch_size, \n",
    "                           epochs=epochs,  \n",
    "                           verbose=1)\n",
    "        # save the model\n",
    "        save_model(model,candidate_class)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CanadianFootballTeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = tf.keras.activations.sigmoid(model4.predict(X_dev)).numpy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
