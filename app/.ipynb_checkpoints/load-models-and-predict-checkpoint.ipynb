{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pattern.text.en import tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/nov-city-ms-project/app/w2v_model/enwiki_model'),\n",
    "#     default='/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(os.getcwd()+\"\\output\\cnn_models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data\n",
    "\n",
    "* data: Tabular data + ground truth\n",
    "* dict_col_candidate_classes: a dictionary with filename_columns and in each of the an array of [(candidate_type, candidate_entity, original_cell_value, rank)]\n",
    "* type_neighours_pos_neg_samples: a dictionary that is used to train the classifiers so for each candidate class we have the neighbouring classes, positive samples from the KG and positive and negative samples from the tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'output\\\\'\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "\n",
    "data = load_json(output_folder+'data.json')\n",
    "dict_col_candidate_classes = load_json(output_folder+'dict_col_candidate_classes.json')\n",
    "type_neighours_pos_neg_samples = load_json(output_folder+'type_neighours_pos_neg_samples.json')\n",
    "dict_cand = load_json(output_folder+'dict_cand.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in type_neighours_pos_neg_samples['SoccerLeague'].keys():\n",
    "    print(f'The length of {key} is: {len(type_neighours_pos_neg_samples[\"Publisher\"][key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "trained_models = list(get_cnn_models(cnn_model_directory))\n",
    "# trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions\n",
    "\n",
    "In this step, provided that we have the ground truth, we asses if the expected class is in the top x of the retrieved candidate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cnn_model_directory, candidate_class):\n",
    "    return keras.models.load_model(cnn_model_directory+'\\%s' % candidate_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get avg number of words per cell value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cell_value_word_lenght(data):\n",
    "    cell_values = list()\n",
    "\n",
    "    for file in data:\n",
    "        for col in data[file]['data']:\n",
    "            cell_values += data[file]['data'][col]\n",
    "\n",
    "    cell_values = list(set(cell_values))\n",
    "    len(cell_values)\n",
    "\n",
    "    word_seq = list()\n",
    "\n",
    "    for cell_value in cell_values:\n",
    "        value = str(cell_value).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(value))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "\n",
    "    return len(word_seq) / len(cell_values)\n",
    "\n",
    "avg_cell_value_word_lenght(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_columns_random(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = list([ent])\n",
    "            unit += random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            \n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def generate_synthetic_columns_sliding_window(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i in range(len(entities)-synthetic_column_size+1):\n",
    "            try:\n",
    "                unit = entities[i:i+synthetic_column_size]\n",
    "            except:\n",
    "                pass\n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = str(ent).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(cell_values):\n",
    "    synthetic_columns = generate_synthetic_columns_sliding_window(cell_values, FLAGS.synthetic_column_size)\n",
    "\n",
    "    synthetic_columns_sequences = list()\n",
    "    for synthetic_column in synthetic_columns:\n",
    "    #     print(synthetic_column)\n",
    "        synthetic_columns_sequences.append(synthetic_columns2sequence(synthetic_column, FLAGS.sequence_size))\n",
    "\n",
    "\n",
    "    X = np.zeros((len(synthetic_columns_sequences), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "\n",
    "    for sample_i, sequence in enumerate(synthetic_columns_sequences):\n",
    "        X[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    return X\n",
    "\n",
    "# X = embedding(unique_cell_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow import keras\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "\n",
    "try:\n",
    "    dict_predictions = load_json(output_folder+'dict_predictions.json')\n",
    "except:\n",
    "    dict_predictions = dict()\n",
    "\n",
    "# for filename in tqdm(dict_cand):\n",
    "for filename in tqdm(dict(itertools.islice(dict_cand.items(), 200))):\n",
    "    if filename not in dict_predictions.keys():\n",
    "        with open(('output/dict_predictions.json'), 'w') as fp:\n",
    "            json.dump(dict_predictions, fp)   \n",
    "\n",
    "        dict_predictions[filename] = dict()\n",
    "        for col in dict_cand[filename]:\n",
    "            dict_predictions[filename][col] = dict()\n",
    "            # Get the candidate classes identified in previous steps. there is a limit variable in case we want to focus on the top x portion of that list\n",
    "            candidate_cls = dict_cand[filename][col]['class_without_hr'][:limit]\n",
    "    #         candidate_cls = trained_models\n",
    "            cell_values = data[filename]['data'][col]\n",
    "\n",
    "            print(filename, col)\n",
    "            actual_cls = data[filename]['gt'][col]\n",
    "            dict_predictions[filename][col]['gt'] = actual_cls\n",
    "            print(actual_cls)\n",
    "\n",
    "            unique_cell_values=list(set(cell_values))\n",
    "            X = embedding(unique_cell_values)\n",
    "\n",
    "            results = list()\n",
    "            for cls in tqdm(candidate_cls):\n",
    "                # load the model\n",
    "                try:\n",
    "                    model = load_model(cnn_model_directory, cls)\n",
    "                    y_pred = tf.keras.activations.sigmoid(model.predict(X)).numpy().round()\n",
    "                    results.append((cls, round(y_pred.sum()*100/X.shape[0],2)))\n",
    "                    tf.keras.backend.clear_session()\n",
    "                except:\n",
    "                    pass\n",
    "#                 print(cls)\n",
    "            results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "            dict_predictions[filename][col]['cand_cls'] = candidate_cls\n",
    "            dict_predictions[filename][col]['pred'] = results\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "with open(('output/dict_predictions-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_predictions, fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbo_sparql_results(query_string):\n",
    "    \n",
    "    classes = list([])\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    \n",
    "    \n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    sparql.setQuery(query_string)\n",
    "    \n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        for entity_class in qres['results']['bindings']:\n",
    "            if dbo_prefix in entity_class[list(qres['results']['bindings'][0].keys())[0]]['value']:\n",
    "                candicate_class = entity_class[list(qres['results']['bindings'][0].keys())[0]]['value'].split(dbo_prefix)[1]\n",
    "                classes.append(candicate_class)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return classes\n",
    "\n",
    "def get_dbo_subclass(superClass):\n",
    "    \n",
    "    query_string = f'''\n",
    "    SELECT distinct ?subClass \n",
    "    WHERE {{ ?subClass rdfs:subClassOf dbo:{superClass}. }}\n",
    "    '''\n",
    "    return dbo_sparql_results(query_string)\n",
    "\n",
    "\n",
    "def get_dbo_superclass(subclass):\n",
    "    \n",
    "    query_string = f'''\n",
    "    SELECT distinct ?superClass \n",
    "    WHERE {{ dbo:{subclass} rdfs:subClassOf ?superClass . }}\n",
    "    '''\n",
    "    \n",
    "    return dbo_sparql_results(query_string)\n",
    "\n",
    "\n",
    "def get_dbo_superclasses(subclass):\n",
    "    classes = list([])\n",
    "    \n",
    "    try:\n",
    "        parent = get_dbo_superclass(subclass)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    while len(parent) > 0:\n",
    "        classes.append(parent[0])\n",
    "        parent = get_dbo_superclass(parent[0])\n",
    "    return classes\n",
    "\n",
    "get_dbo_superclasses('Actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_assessment(dict_cand, threshold = 500):\n",
    "    found = 0\n",
    "    found_cnn = 0\n",
    "    total_columns = 0\n",
    "    parent_found = 0\n",
    "    parent_found_in_lookup = 0\n",
    "\n",
    "    for file in dict_cand:\n",
    "        for col in dict_cand[file]:\n",
    "#             print (file, col)\n",
    "            candidate_class_lookup = dict_cand[file][col]['cand_cls'][:threshold]\n",
    "            candidate_class_cnn = [x[0] for x in dict_cand[file][col]['pred'][:threshold]]\n",
    "            actual_cls = dict_cand[file][col]['gt']\n",
    "#             print(candidate_class_lookup)\n",
    "#             print (candidate_class_cnn)\n",
    "#             print (actual_cls)\n",
    "            if actual_cls in candidate_class_lookup:\n",
    "                found+=1\n",
    "                parent_found_in_lookup += 1\n",
    "            \n",
    "            # else we give half a point in case the predicted value is in the hierarchy (i.e. parent) of the actual value\n",
    "            else:\n",
    "                parents_of_expected_type = get_dbo_superclasses(actual_cls)\n",
    "                intersection = [value for value in parents_of_expected_type if value in candidate_class_lookup]\n",
    "                if len(intersection) > 0:\n",
    "                    parent_found_in_lookup += 0.5\n",
    "\n",
    "            if actual_cls in candidate_class_cnn:\n",
    "                found_cnn+=1\n",
    "                parent_found += 1\n",
    "            \n",
    "            # else we give half a point in case the predicted value is in the hierarchy (i.e. parent) of the actual value\n",
    "            else:\n",
    "                parents_of_expected_type = get_dbo_superclasses(actual_cls)\n",
    "                intersection = [value for value in parents_of_expected_type if value in candidate_class_cnn]\n",
    "                if len(intersection) > 0:\n",
    "                    parent_found += 0.5\n",
    "                    \n",
    "            total_columns+=1\n",
    "    return (round(100*found/total_columns,2),round(100*parent_found_in_lookup/total_columns,2),round(100*found_cnn/total_columns,2),round(100*parent_found/total_columns,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions_top5_random = load_json(output_folder+'dict_predictions_top5_random.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# first we calculate the max number of candidates across all the columns in the tabular data so we cap the range\n",
    "max_cand = 0\n",
    "for file in dict_predictions_top5_random:\n",
    "    for col in dict_predictions_top5_random[file]:\n",
    "        \n",
    "        candidate_cls = set()\n",
    "        for element in dict_predictions_top5_random[file][col]:\n",
    "            candidate_cls.add(element[0])\n",
    "        if len(candidate_cls) > max_cand:\n",
    "            max_cand = len(candidate_cls)\n",
    "\n",
    "            \n",
    "# the we increase the size of top list one element at a time until we reac the cap...\n",
    "for i in range(1,6):\n",
    "    (x,y) = (i, lookup_assessment(dict_predictions_top5_random, i))\n",
    "    results.append((x,y))\n",
    "\n",
    "#... and plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x[0] for x in results],[x[1] for x in results])\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions_top5_sliding = load_json(output_folder+'dict_predictions_top5_sliding.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# # first we calculate the max number of candidates across all the columns in the tabular data so we cap the range\n",
    "# max_cand = 0\n",
    "# for file in dict_predictions:\n",
    "#     for col in dict_predictions[file]:\n",
    "        \n",
    "#         candidate_cls = set()\n",
    "#         for element in dict_predictions[file][col]:\n",
    "#             candidate_cls.add(element[0])\n",
    "#         if len(candidate_cls) > max_cand:\n",
    "#             max_cand = len(candidate_cls)\n",
    "\n",
    "            \n",
    "# the we increase the size of top list one element at a time until we reac the cap...\n",
    "for i in range(1,6):\n",
    "    (x,y) = (i, lookup_assessment(dict_predictions_top5_sliding, i))\n",
    "    results.append((x,y))\n",
    "\n",
    "#... and plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x[0] for x in results],[x[1] for x in results])\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(('output/dict_predictions.json'), 'w') as fp:\n",
    "        json.dump(dict_predictions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions_top5_sliding['50245608_0_871275842592178099']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_predictions_top5_sliding['50245608_0_871275842592178099']['0']['gt'])\n",
    "\n",
    "for cls in dict_predictions_top5_sliding['50245608_0_871275842592178099']['0']['pred']:\n",
    "    print(cls)\n",
    "    print(f\"Parents: {get_dbo_superclasses(cls[0])}\")\n",
    "    print(f\"Offspring: {get_dbo_subclass(cls[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_predictions_top5_sliding['8468806_0_4382447409703007384']['1']['gt'])\n",
    "\n",
    "for filename in dict(itertools.islice(dict_predictions_top5_sliding.items(), 10)):\n",
    "\n",
    "    index = 0\n",
    "    print(f\"-----------------------------------------------\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    for col in dict_predictions_top5_sliding[filename]:\n",
    "        actual_cls = dict_predictions_top5_sliding[filename][col]['gt']\n",
    "        top1_clc = dict_predictions_top5_sliding[filename][col]['pred'][0][0]\n",
    "        col_title = data[filename]['column_titles'][index]\n",
    "        index += 1\n",
    "        print(f\"Column: {col},Actual class: {actual_cls}, Predicted Class: {top1_clc}, Column Title: {col_title}\")\n",
    "        print(f\"Parents: {get_dbo_superclasses(cls[0])}\")\n",
    "        print(f\"Offspring: {get_dbo_subclass(cls[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dbo_superclasses('Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dbo_subclass('Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all folders under the cnn_models root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "trained_models = list(get_cnn_models(cnn_model_directory))\n",
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
