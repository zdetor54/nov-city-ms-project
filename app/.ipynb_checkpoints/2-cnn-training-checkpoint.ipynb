{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'output\\\\'\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "\n",
    "# data = load_json(output_folder+'data.json')\n",
    "# dict_col_candidate_classes = load_json(output_folder+'dict_col_candidate_classes.json')\n",
    "type_neighours_pos_neg_samples = load_json(output_folder+'type_neighours_pos_neg_samples.json')\n",
    "# dict_cand = load_json(output_folder+'dict_cand.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cell_value_word_lenght(data):\n",
    "    cell_values = list()\n",
    "\n",
    "    for file in data:\n",
    "        for col in data[file]['data']:\n",
    "            cell_values += data[file]['data'][col]\n",
    "\n",
    "    cell_values = list(set(cell_values))\n",
    "    len(cell_values)\n",
    "\n",
    "    word_seq = list()\n",
    "\n",
    "    for cell_value in cell_values:\n",
    "        value = str(cell_value).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(value))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "\n",
    "    return len(word_seq) / len(cell_values)\n",
    "\n",
    "# avg_cell_value_word_lenght(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=30,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/nov-city-ms-project/app/w2v_model/enwiki_model'),\n",
    "#     default='/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONLY LOAD ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.text.en import tokenize\n",
    "\n",
    "def generate_synthetic_columns(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            unit.append(ent)\n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = ent.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_samples(pos, neg, pct = 0.5):\n",
    "    if len(pos) <= len(neg):\n",
    "        return pos+[random.choice(pos) for _ in range(math.ceil((len(neg)-len(pos))*pct))], neg\n",
    "    else:\n",
    "        return pos, neg+[random.choice(neg) for _ in range(math.ceil((len(pos)-len(neg))*pct))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(entities_positive, entities_negative):\n",
    "    # embedding\n",
    "    units_positive = generate_synthetic_columns(entities_positive, FLAGS.synthetic_column_size)\n",
    "    units_negative = generate_synthetic_columns(entities_negative, FLAGS.synthetic_column_size)\n",
    "\n",
    "    sequences_positive = list()\n",
    "    for ent_unit in units_positive:\n",
    "        sequences_positive.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "    sequences_negative = list()\n",
    "    for ent_unit in units_negative:\n",
    "        sequences_negative.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "\n",
    "    x = np.zeros((len(sequences_positive) + len(sequences_negative), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for sample_i, sequence in enumerate(sequences_positive + sequences_negative):\n",
    "        x[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    y_positive = np.ones((len(sequences_positive), 1))\n",
    "    y_negative = np.zeros((len(sequences_negative), 1))\n",
    "    y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "    # shuffling\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    return x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the directory of cnn trained models so that the solution can pick up from where it left off in case there are more classifiers to be trained based on the candidate classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "\n",
    "batch_size = 32 \n",
    "epochs = 2\n",
    "test_train_split = 0.2\n",
    "\n",
    "def save_model(model, candidate_class):\n",
    "    cwd = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "    model.save(cwd+'/%s' % candidate_class)\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')\n",
    "\n",
    "for candidate_class in tqdm(type_neighours_pos_neg_samples):\n",
    "    if candidate_class not in loaded_models:\n",
    "        print(candidate_class)\n",
    "        # Get the positive and negative samples to train the model\n",
    "        cls_neg_par_entities = list(type_neighours_pos_neg_samples[candidate_class]['negative_candidate_entities'])\n",
    "        cls_pos_gen_entities = list(type_neighours_pos_neg_samples[candidate_class]['general_positive_entities'])\n",
    "        \n",
    "        # had to add this as I was running out of memory\n",
    "        if len(cls_neg_par_entities)>10000:\n",
    "            cls_neg_par_entities = random.sample(cls_neg_par_entities, 10000)\n",
    "            try: \n",
    "                cls_pos_gen_entities = random.sample(cls_pos_gen_entities, 10000)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(len(cls_pos_gen_entities), len(cls_neg_par_entities))\n",
    "        # align the samples to create a balance set\n",
    "        p_ents, n_ents = align_samples(cls_pos_gen_entities, cls_neg_par_entities,1)\n",
    "        \n",
    "        print(len(p_ents), len(n_ents))\n",
    "        \n",
    "        # Create the embeddings using the w2v_model. here the samples are shuffled so we have a mixture of positive and negative samples\n",
    "        X, Y = embedding(p_ents, n_ents)\n",
    "        \n",
    "\n",
    "        dev_sample_index = int(test_train_split * float(X.shape[0]))\n",
    "        X_train, X_dev = X[dev_sample_index:], X[:dev_sample_index]\n",
    "        Y_train, Y_dev = Y[dev_sample_index:], Y[:dev_sample_index]\n",
    "\n",
    "        HEIGHT = X_train.shape[1]\n",
    "        WIDTH = X_train.shape[2]\n",
    "\n",
    "        #Build the model\n",
    "        model = Sequential([\n",
    "            Conv2D(16, 3, padding='same', activation='relu', \n",
    "                   input_shape=(HEIGHT, WIDTH ,1)),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        # print the model architecture\n",
    "    #     model.summary()\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                           batch_size=batch_size, \n",
    "                           epochs=epochs,  \n",
    "                           verbose=2)\n",
    "        # save the model\n",
    "        save_model(model,candidate_class)\n",
    "        tf.keras.backend.clear_session()\n",
    "        clear_output(wait=True)\n",
    "# 11483 11483 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CanadianFootballTeam\n",
    "\n",
    "# WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.294255). Check your callbacks.\n",
    "---2021----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in type_neighours_pos_neg_samples['Organisation'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
