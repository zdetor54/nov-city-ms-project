{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "# from collections import Counter\n",
    "# import time\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pattern.text.en import tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/nov-city-ms-project/app/w2v_model/enwiki_model'),\n",
    "#     default='/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(os.getcwd()+\"\\output\\cnn_models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data\n",
    "\n",
    "* data: Tabular data + ground truth\n",
    "* dict_col_candidate_classes: a dictionary with filename_columns and in each of the an array of [(candidate_type, candidate_entity, original_cell_value, rank)]\n",
    "* type_neighours_pos_neg_samples: a dictionary that is used to train the classifiers so for each candidate class we have the neighbouring classes, positive samples from the KG and positive and negative samples from the tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'output\\\\'\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "\n",
    "data = load_json(output_folder+'data-rep.json')\n",
    "# dict_col_candidate_classes = load_json(output_folder+'dict_col_candidate_classes.json')\n",
    "type_neighours_pos_neg_samples = load_json(output_folder+'type_neighours_pos_neg_samples.json')\n",
    "dict_cand = load_json(output_folder+'dict_cand.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "trained_models = list(get_cnn_models(cnn_model_directory))\n",
    "# trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions\n",
    "\n",
    "In this step, provided that we have the ground truth, we asses if the expected class is in the top x of the retrieved candidate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cnn_model_directory, candidate_class):\n",
    "    return keras.models.load_model(cnn_model_directory+'\\%s' % candidate_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get avg number of words per cell value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.504999425353408"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_cell_value_word_lenght(data):\n",
    "    cell_values = list()\n",
    "\n",
    "    for file in data:\n",
    "        for col in file['data']:\n",
    "            cell_values += file['data'][col]\n",
    "\n",
    "    cell_values = list(set(cell_values))\n",
    "    len(cell_values)\n",
    "\n",
    "    word_seq = list()\n",
    "\n",
    "    for cell_value in cell_values:\n",
    "        value = str(cell_value).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(value))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "\n",
    "    return len(word_seq) / len(cell_values)\n",
    "\n",
    "avg_cell_value_word_lenght(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_columns(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = list([ent])\n",
    "            unit += random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            \n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = str(ent).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to delete align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(cell_values):\n",
    "    synthetic_columns = generate_synthetic_columns(cell_values, FLAGS.synthetic_column_size)\n",
    "\n",
    "    synthetic_columns_sequences = list()\n",
    "    for synthetic_column in synthetic_columns:\n",
    "    #     print(synthetic_column)\n",
    "        synthetic_columns_sequences.append(synthetic_columns2sequence(synthetic_column, FLAGS.sequence_size))\n",
    "\n",
    "\n",
    "    X = np.zeros((len(synthetic_columns_sequences), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "\n",
    "    for sample_i, sequence in enumerate(synthetic_columns_sequences):\n",
    "        X[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    return X\n",
    "\n",
    "# X = embedding(unique_cell_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow import keras\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.38s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9fe8ca354788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output/dict_predictions-%s.json'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "limit = 5\n",
    "\n",
    "dict_predictions = dict()\n",
    "\n",
    "# for filename in tqdm(dict_cand):\n",
    "for filename in tqdm(dict(itertools.islice(dict_cand.items(), 1))):\n",
    "    dict_predictions[filename] = dict()\n",
    "    for col in dict_cand[filename]:\n",
    "        dict_predictions[filename][col] = dict()\n",
    "        # Get the candidate classes identified in previous steps. there is a limit variable in case we want to focus on the top x portion of that list\n",
    "        candidate_cls = dict_cand[filename][col]['class_without_hr'][:limit]\n",
    "#         candidate_cls = trained_models\n",
    "        cell_values = next(item for item in data if item[\"filename\"] == filename)['data'][col]\n",
    "        \n",
    "        print(filename, col, candidate_cls)\n",
    "        actual_cls = next(item for item in data if item[\"filename\"] == filename)['gt'][col]\n",
    "        dict_predictions[filename][col]['gt'] = actual_cls\n",
    "        print(actual_cls)\n",
    "        \n",
    "        unique_cell_values=list(set(cell_values))\n",
    "        X = embedding(unique_cell_values)\n",
    "        \n",
    "        results = list()\n",
    "        for cls in candidate_cls:\n",
    "            # load the model\n",
    "            model = load_model(cnn_model_directory, cls)\n",
    "            y_pred = tf.keras.activations.sigmoid(model.predict(X)).numpy().round()\n",
    "            results.append((cls, round(y_pred.sum()*100/X.shape[0],2)))\n",
    "#             print(cls)\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        dict_predictions[filename][col]['cand_cls'] = candidate_cls\n",
    "        dict_predictions[filename][col]['pred'] = results\n",
    "            \n",
    "    clear_output(wait=True)   \n",
    "with open(('output/dict_predictions-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_predictions, fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TelevisionShow', 56.0),\n",
       " ('Group', 56.0),\n",
       " ('Book', 46.0),\n",
       " ('Film', 39.0),\n",
       " ('Work', 39.0),\n",
       " ('TelevisionEpisode', 39.0),\n",
       " ('HollywoodCartoon', 38.0),\n",
       " ('Comic', 34.0),\n",
       " ('Play', 31.0),\n",
       " ('SpaceMission', 30.0),\n",
       " ('Poem', 28.0),\n",
       " ('Band', 28.0),\n",
       " ('WrittenWork', 26.0),\n",
       " ('Protein', 26.0),\n",
       " ('Manga', 26.0),\n",
       " ('Software', 23.0),\n",
       " ('VideoGame', 20.0),\n",
       " ('Horse', 19.0),\n",
       " ('WineRegion', 17.0),\n",
       " ('Brain', 16.0),\n",
       " ('Musical', 13.0),\n",
       " ('Mammal', 13.0),\n",
       " ('Artwork', 12.0),\n",
       " ('RadioProgram', 10.0),\n",
       " ('Website', 9.0),\n",
       " ('Bird', 8.0),\n",
       " ('WorldHeritageSite', 8.0),\n",
       " ('Activity', 7.0),\n",
       " ('Single', 7.0),\n",
       " ('Album', 7.0),\n",
       " ('Grape', 7.0),\n",
       " ('EthnicGroup', 6.0),\n",
       " ('TimeInterval', 6.0),\n",
       " ('NaturalPlace', 6.0),\n",
       " ('RollerCoaster', 6.0),\n",
       " ('Fashion', 6.0),\n",
       " ('Beverage', 6.0),\n",
       " ('Colour', 5.0),\n",
       " ('SupremeCourtOfTheUnitedStatesCase', 5.0),\n",
       " ('Food', 5.0),\n",
       " ('Mineral', 5.0),\n",
       " ('Settlement', 4.0),\n",
       " ('PopulatedPlace', 4.0),\n",
       " ('AnatomicalStructure', 4.0),\n",
       " ('ComedyGroup', 4.0),\n",
       " ('Currency', 4.0),\n",
       " ('Weapon', 4.0),\n",
       " ('TopicalConcept', 4.0),\n",
       " ('Newspaper', 4.0),\n",
       " ('Device', 3.0),\n",
       " ('Swimmer', 3.0),\n",
       " ('Train', 3.0),\n",
       " ('Stream', 3.0),\n",
       " ('Place', 3.0),\n",
       " ('Gene', 3.0),\n",
       " ('Game', 3.0),\n",
       " ('ProtectedArea', 3.0),\n",
       " ('Species', 3.0),\n",
       " ('Company', 2.0),\n",
       " ('Broadcaster', 2.0),\n",
       " ('ClassicalMusicComposition', 2.0),\n",
       " ('Drug', 2.0),\n",
       " ('ComicsCharacter', 2.0),\n",
       " ('MusicalWork', 2.0),\n",
       " ('Automobile', 2.0),\n",
       " ('BasketballTeam', 2.0),\n",
       " ('FictionalCharacter', 2.0),\n",
       " ('Fish', 2.0),\n",
       " ('ComicStrip', 2.0),\n",
       " ('MusicGenre', 2.0),\n",
       " ('RecordLabel', 2.0),\n",
       " ('Planet', 2.0),\n",
       " ('Eukaryote', 2.0),\n",
       " ('Saint', 2.0),\n",
       " ('RadioStation', 2.0),\n",
       " ('City', 2.0),\n",
       " ('Genre', 2.0),\n",
       " ('GivenName', 1.0),\n",
       " ('ChemicalSubstance', 1.0),\n",
       " ('AmusementParkAttraction', 1.0),\n",
       " ('Insect', 1.0),\n",
       " ('WrestlingEvent', 1.0),\n",
       " ('Sport', 1.0),\n",
       " ('MythologicalFigure', 1.0),\n",
       " ('HumanGene', 1.0),\n",
       " ('Holiday', 1.0),\n",
       " ('Motorcycle', 1.0),\n",
       " ('Crustacean', 1.0),\n",
       " ('Magazine', 1.0),\n",
       " ('Convention', 1.0),\n",
       " ('Mollusca', 1.0),\n",
       " ('Arachnid', 1.0),\n",
       " ('HandballPlayer', 1.0),\n",
       " ('Song', 1.0),\n",
       " ('Disease', 1.0),\n",
       " ('Plant', 1.0),\n",
       " ('ProgrammingLanguage', 1.0),\n",
       " ('ChemicalCompound', 1.0),\n",
       " ('Animal', 1.0),\n",
       " ('Artist', 1.0),\n",
       " ('ArtificialSatellite', 1.0),\n",
       " ('Bone', 1.0),\n",
       " ('Biomolecule', 1.0),\n",
       " ('Ligament', 0.0),\n",
       " ('LacrossePlayer', 0.0),\n",
       " ('Event', 0.0),\n",
       " ('MilitaryUnit', 0.0),\n",
       " ('Person', 0.0),\n",
       " ('Rower', 0.0),\n",
       " ('Cleric', 0.0),\n",
       " ('Race', 0.0),\n",
       " ('InformationAppliance', 0.0),\n",
       " ('Priest', 0.0),\n",
       " ('MountainRange', 0.0),\n",
       " ('EducationalInstitution', 0.0),\n",
       " ('SportsClub', 0.0),\n",
       " ('Tower', 0.0),\n",
       " ('Park', 0.0),\n",
       " ('WinterSportPlayer', 0.0),\n",
       " ('FootballMatch', 0.0),\n",
       " ('MilitaryConflict', 0.0),\n",
       " ('Organisation', 0.0),\n",
       " ('TennisPlayer', 0.0),\n",
       " ('Mountain', 0.0),\n",
       " ('ReligiousBuilding', 0.0),\n",
       " ('Station', 0.0),\n",
       " ('PowerStation', 0.0),\n",
       " ('Fungus', 0.0),\n",
       " ('WomensTennisAssociationTournament', 0.0),\n",
       " ('SoccerManager', 0.0),\n",
       " ('BodyOfWater', 0.0),\n",
       " ('Airport', 0.0),\n",
       " ('University', 0.0),\n",
       " ('AmericanFootballTeam', 0.0),\n",
       " ('Continent', 0.0),\n",
       " ('Name', 0.0),\n",
       " ('Stadium', 0.0),\n",
       " ('Pope', 0.0),\n",
       " ('IceHockeyPlayer', 0.0),\n",
       " ('Dam', 0.0),\n",
       " ('CombinationDrug', 0.0),\n",
       " ('FormulaOneTeam', 0.0),\n",
       " ('SoccerClubSeason', 0.0),\n",
       " ('PlayboyPlaymate', 0.0),\n",
       " ('SocietalEvent', 0.0),\n",
       " ('Presenter', 0.0),\n",
       " ('CollegeCoach', 0.0),\n",
       " ('Case', 0.0),\n",
       " ('Town', 0.0),\n",
       " ('RoadTunnel', 0.0),\n",
       " ('AustralianRulesFootballPlayer', 0.0),\n",
       " ('FootballLeagueSeason', 0.0),\n",
       " ('BusCompany', 0.0),\n",
       " ('SportsTeamSeason', 0.0),\n",
       " ('PeriodicalLiterature', 0.0),\n",
       " ('Senator', 0.0),\n",
       " ('SportsManager', 0.0),\n",
       " ('Athlete', 0.0),\n",
       " ('Venue', 0.0),\n",
       " ('FloweringPlant', 0.0),\n",
       " ('CyclingRace', 0.0),\n",
       " ('Library', 0.0),\n",
       " ('GolfPlayer', 0.0),\n",
       " ('MilitaryPerson', 0.0),\n",
       " ('GolfTournament', 0.0),\n",
       " ('UnitOfWork', 0.0),\n",
       " ('Bank', 0.0),\n",
       " ('CultivatedVariety', 0.0),\n",
       " ('Rocket', 0.0),\n",
       " ('Reptile', 0.0),\n",
       " ('Archbishop', 0.0),\n",
       " ('GridironFootballPlayer', 0.0),\n",
       " ('TelevisionStation', 0.0),\n",
       " ('AcademicJournal', 0.0),\n",
       " ('FigureSkater', 0.0),\n",
       " ('SoccerTournament', 0.0),\n",
       " ('Lighthouse', 0.0),\n",
       " ('Canal', 0.0),\n",
       " ('Noble', 0.0),\n",
       " ('Actor', 0.0),\n",
       " ('BasketballPlayer', 0.0),\n",
       " ('HorseRace', 0.0),\n",
       " ('Legislature', 0.0),\n",
       " ('Infrastructure', 0.0),\n",
       " ('Religious', 0.0),\n",
       " ('HockeyTeam', 0.0),\n",
       " ('SiteOfSpecialScientificInterest', 0.0),\n",
       " ('RouteOfTransportation', 0.0),\n",
       " ('Award', 0.0),\n",
       " ('HistoricBuilding', 0.0),\n",
       " ('AdultActor', 0.0),\n",
       " ('ClericalAdministrativeRegion', 0.0),\n",
       " ('River', 0.0),\n",
       " ('SportsEvent', 0.0),\n",
       " ('Architect', 0.0),\n",
       " ('College', 0.0),\n",
       " ('BasketballLeague', 0.0),\n",
       " ('PoliticalParty', 0.0),\n",
       " ('Entomologist', 0.0),\n",
       " ('Philosopher', 0.0),\n",
       " ('Politician', 0.0),\n",
       " ('GaelicGamesPlayer', 0.0),\n",
       " ('Ship', 0.0),\n",
       " ('Village', 0.0),\n",
       " ('MartialArtist', 0.0),\n",
       " ('MedicalSpecialty', 0.0),\n",
       " ('Amphibian', 0.0),\n",
       " ('Racecourse', 0.0),\n",
       " ('AdministrativeRegion', 0.0),\n",
       " ('Royalty', 0.0),\n",
       " ('Bay', 0.0),\n",
       " ('PrimeMinister', 0.0),\n",
       " ('BaseballLeague', 0.0),\n",
       " ('SoccerClub', 0.0),\n",
       " ('Astronaut', 0.0),\n",
       " ('Coach', 0.0),\n",
       " ('CityDistrict', 0.0),\n",
       " ('ChristianPatriarch', 0.0),\n",
       " ('Conifer', 0.0),\n",
       " ('Historian', 0.0),\n",
       " ('Poet', 0.0),\n",
       " ('LegalCase', 0.0),\n",
       " ('GovernmentAgency', 0.0),\n",
       " ('Governor', 0.0),\n",
       " ('Engineer', 0.0),\n",
       " ('Criminal', 0.0),\n",
       " ('Airline', 0.0),\n",
       " ('Road', 0.0),\n",
       " ('FormulaOneRacer', 0.0),\n",
       " ('Year', 0.0),\n",
       " ('IceHockeyLeague', 0.0),\n",
       " ('MeanOfTransportation', 0.0),\n",
       " ('Anime', 0.0),\n",
       " ('Garden', 0.0),\n",
       " ('School', 0.0),\n",
       " ('RugbyPlayer', 0.0),\n",
       " ('ArchitecturalStructure', 0.0),\n",
       " ('MilitaryStructure', 0.0),\n",
       " ('Hospital', 0.0),\n",
       " ('BaseballTeam', 0.0),\n",
       " ('HistoricPlace', 0.0),\n",
       " ('Hotel', 0.0),\n",
       " ('Prison', 0.0),\n",
       " ('Theatre', 0.0),\n",
       " ('ComicsCreator', 0.0),\n",
       " ('MountainPass', 0.0),\n",
       " ('Cartoon', 0.0),\n",
       " ('TelevisionSeason', 0.0),\n",
       " ('Bacteria', 0.0),\n",
       " ('Tournament', 0.0),\n",
       " ('Locomotive', 0.0),\n",
       " ('ArtistDiscography', 0.0),\n",
       " ('Curler', 0.0),\n",
       " ('Agent', 0.0),\n",
       " ('GolfLeague', 0.0),\n",
       " ('MotorsportRacer', 0.0),\n",
       " ('SportsSeason', 0.0),\n",
       " ('RailwayLine', 0.0),\n",
       " ('Bodybuilder', 0.0),\n",
       " ('Chef', 0.0),\n",
       " ('Comedian', 0.0),\n",
       " ('Olympics', 0.0),\n",
       " ('CricketTeam', 0.0),\n",
       " ('OfficeHolder', 0.0),\n",
       " ('Election', 0.0),\n",
       " ('Island', 0.0),\n",
       " ('Wrestler', 0.0),\n",
       " ('Bridge', 0.0),\n",
       " ('Lake', 0.0),\n",
       " ('SoccerLeague', 0.0),\n",
       " ('Building', 0.0),\n",
       " ('Outbreak', 0.0),\n",
       " ('PublicTransitSystem', 0.0),\n",
       " ('Writer', 0.0),\n",
       " ('Boxer', 0.0),\n",
       " ('MemberOfParliament', 0.0),\n",
       " ('Aircraft', 0.0),\n",
       " ('PersonFunction', 0.0),\n",
       " ('Winery', 0.0),\n",
       " ('Cardinal', 0.0),\n",
       " ('Economist', 0.0),\n",
       " ('SportsLeague', 0.0),\n",
       " ('Country', 0.0),\n",
       " ('Castle', 0.0),\n",
       " ('Language', 0.0),\n",
       " ('BaseballPlayer', 0.0),\n",
       " ('RugbyClub', 0.0),\n",
       " ('SoapCharacter', 0.0),\n",
       " ('MusicalArtist', 0.0),\n",
       " ('Region', 0.0),\n",
       " ('ScreenWriter', 0.0),\n",
       " ('SoccerPlayer', 0.0),\n",
       " ('Museum', 0.0),\n",
       " ('Galaxy', 0.0),\n",
       " ('RacingDriver', 0.0),\n",
       " ('SportsTeamMember', 0.0),\n",
       " ('Scientist', 0.0),\n",
       " ('Cricketer', 0.0),\n",
       " ('Diocese', 0.0),\n",
       " ('Cyclist', 0.0),\n",
       " ('Fern', 0.0),\n",
       " ('Congressman', 0.0),\n",
       " ('President', 0.0),\n",
       " ('CelestialBody', 0.0),\n",
       " ('Monument', 0.0),\n",
       " ('SportsTeam', 0.0),\n",
       " ('TennisTournament', 0.0),\n",
       " ('NationalFootballLeagueSeason', 0.0),\n",
       " ('AmericanFootballPlayer', 0.0),\n",
       " ('Location', 0.0),\n",
       " ('TimePeriod', 0.0),\n",
       " ('BroadcastNetwork', 0.0),\n",
       " ('MotorsportSeason', 0.0),\n",
       " ('ChristianBishop', 0.0),\n",
       " ('Publisher', 0.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_predictions['58891288_0_1117541047012405958']['1']['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(('output/dict_predictions-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_predictions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all folders under the cnn_models root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "trained_models = list(get_cnn_models(cnn_model_directory))\n",
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
