{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(os.getcwd()+\"\\output\\cnn_models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data\n",
    "\n",
    "* data: Tabular data + ground truth\n",
    "* dict_col_candidate_classes: a dictionary with filename_columns and in each of the an array of [(candidate_type, candidate_entity, original_cell_value, rank)]\n",
    "* type_neighours_pos_neg_samples: a dictionary that is used to train the classifiers so for each candidate class we have the neighbouring classes, positive samples from the KG and positive and negative samples from the tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only consider candidate classes between columns that have links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"output\\\\\"\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "\n",
    "data = load_json(output_folder+'data.json')\n",
    "\n",
    "dict_col_candidate_classes = load_json(output_folder+'dict_col_candidate_classes.json')\n",
    "\n",
    "\n",
    "try:\n",
    "    type_neighours_pos_neg_samples = load_json(output_folder+'type_neighours_pos_neg_samples.json')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    dict_cand = load_json(output_folder+'dict_cand.json')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess lookup efficiency\n",
    "\n",
    "In this step, provided that we have the ground truth, we asses if the expected class is in the top x of the retrieved candidate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbo_sparql_results(query_string):\n",
    "    \n",
    "    classes = list([])\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    \n",
    "    \n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    sparql.setQuery(query_string)\n",
    "    \n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        for entity_class in qres['results']['bindings']:\n",
    "            if dbo_prefix in entity_class[list(qres['results']['bindings'][0].keys())[0]]['value']:\n",
    "                candicate_class = entity_class[list(qres['results']['bindings'][0].keys())[0]]['value'].split(dbo_prefix)[1]\n",
    "                classes.append(candicate_class)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return classes\n",
    "\n",
    "def get_dbo_subclass(superClass):\n",
    "    \n",
    "    query_string = f'''\n",
    "    SELECT distinct ?subClass \n",
    "    WHERE {{ ?subClass rdfs:subClassOf dbo:{superClass}. }}\n",
    "    '''\n",
    "    return dbo_sparql_results(query_string)\n",
    "\n",
    "\n",
    "def get_dbo_superclass(subclass):\n",
    "    \n",
    "    query_string = f'''\n",
    "    SELECT distinct ?superClass \n",
    "    WHERE {{ dbo:{subclass} rdfs:subClassOf ?superClass . }}\n",
    "    '''\n",
    "    \n",
    "    return dbo_sparql_results(query_string)\n",
    "\n",
    "\n",
    "def get_dbo_superclasses(subclass):\n",
    "    classes = list([])\n",
    "    \n",
    "    try:\n",
    "        parent = get_dbo_superclass(subclass)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    while len(parent) > 0:\n",
    "        classes.append(parent[0])\n",
    "        parent = get_dbo_superclass(parent[0])\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Work']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dbo_superclass('Film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_candidate_classes(dict_col_candidate_classes,filename,col,limit = -1):\n",
    "    if limit >= 0:\n",
    "        candidate_list = Counter([i[0] for i in dict_col_candidate_classes[filename][col]]).most_common()[:limit]\n",
    "    else:\n",
    "        candidate_list = Counter([i[0] for i in dict_col_candidate_classes[filename][col]]).most_common()\n",
    "#     return [x[0] for x in candidate_list]\n",
    "    return candidate_list\n",
    "\n",
    "dict_cand = dict()\n",
    "\n",
    "total = 0\n",
    "for filename in dict_col_candidate_classes:\n",
    "    dict_cand[filename] = dict()\n",
    "    for col in dict_col_candidate_classes[filename]:\n",
    "        dict_cand[filename][col] = dict()\n",
    "        dict_cand[filename][col]['class_without_hr'] = get_column_candidate_classes(dict_col_candidate_classes,filename, col)\n",
    "        total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of having a list of all candidate classes that appear in a column, the next step is to remove classes that are higher up in the list if at least one of their subclasses also appears in the list.\n",
    "This approach seems to improve the precision (i.e. the top classes are more relevant) but decrease the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_entities = pd.DataFrame()\n",
    "\n",
    "rank_threshold = 5\n",
    "\n",
    "for filename in dict_col_candidate_classes:\n",
    "    for col in dict_col_candidate_classes[filename]:\n",
    "        df_entities = df_entities.append(pd.DataFrame([[filename,col,x[0], x[1], x[2], x[3]] for x in dict_col_candidate_classes[filename][col]], columns=['filename', 'col','type', 'entity', 'cell_value', 'rank']))\n",
    "        \n",
    "df_entities = df_entities[df_entities['rank'] <= rank_threshold]\n",
    "temp = df_entities.groupby(by=[\"filename\", \"col\", \"type\"]).count()[\"entity\"].reset_index().sort_values(by=['filename', 'col', 'entity'],ascending=False)\n",
    "\n",
    "dict_candidate = dict()\n",
    "for filename in list(temp.filename.unique()):\n",
    "    dict_candidate[filename] = dict()\n",
    "    for col in list(temp[temp.filename == filename].col.unique()):\n",
    "        total = temp[(temp.filename==filename) & (temp.col == col)].sum().entity\n",
    "        df = temp[(temp.filename==filename) & (temp.col == col)].copy()\n",
    "        df['vote_pct'] = df.apply(lambda x : x.entity *100 / total, axis=1)\n",
    "        dict_candidate[filename][col] = dict()\n",
    "        dict_candidate[filename][col]['class_without_hr'] = list(df[['type','entity','vote_pct']].to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for filename in tqdm(dict_cand):\n",
    "    for col in dict_cand[filename]:\n",
    "        dict_cand[filename][col]['class_with_hr'] = list([])\n",
    "        for cls in dict_cand[filename][col]['class_without_hr']:\n",
    "            if len([value for value in get_dbo_subclass(cls) if value in dict_cand[filename][col]['class_without_hr']]) == 0:\n",
    "                dict_cand[filename][col]['class_with_hr'].append(cls)\n",
    "                \n",
    "with open(('output/dict_cand-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_cand, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments:\n",
    " 1. Top X without repetition of cell values, without checking parent child relationships\n",
    " 2. Top x without repetition of cell values, having removed candidate classes for which at list one subclass also appears in the list of candidate classes\n",
    " 3. Top x with repetition of cell values. This means if a value appears multiple times in a column then the cancidate class will get more that one votes from that cell value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_assessment(dict_cand, threshold = 10000):\n",
    "    found = 0\n",
    "    found_with = 0\n",
    "    total_columns = 0\n",
    "\n",
    "    for file in dict_cand:\n",
    "        for col in dict_cand[file]:\n",
    "            candidate_class_without_hr = dict_cand[file][col]['class_without_hr'][:threshold]\n",
    "#             candidate_class_with_hr = dict_cand[file][col]['class_with_hr'][:threshold]\n",
    "            actual_cls = data[file]['gt'][col]\n",
    "            if actual_cls in candidate_class_without_hr:\n",
    "                found+=1\n",
    "#             if actual_cls in candidate_class_with_hr:\n",
    "#                 found_with+=1\n",
    "            total_columns+=1\n",
    "    return (round(100*found/total_columns,2),round(100*found_with/total_columns,2))\n",
    "\n",
    "\n",
    "\n",
    "def lookup_assessment_considering_hr(dict_cand, threshold = 10000):\n",
    "    \"\"\"\n",
    "    This is a function that considers half a point (instead of a whole point) in case the predicted class is a parent of the expected type\n",
    "    \"\"\"\n",
    "    found = 0\n",
    "    found_with = 0\n",
    "    parent_found = 0\n",
    "    total_columns = 0\n",
    "\n",
    "    for file in dict_cand:\n",
    "        for col in dict_cand[file]:\n",
    "            candidate_class_without_hr = dict_cand[file][col]['class_without_hr'][:threshold]\n",
    "#             candidate_class_with_hr = dict_cand[file][col]['class_with_hr'][:threshold]\n",
    "            actual_cls = data[file]['gt'][col]\n",
    "            if actual_cls in candidate_class_without_hr:\n",
    "                found+=1\n",
    "                parent_found += 1\n",
    "            # else we give half a point in case the predicted value is in the hierarchy (i.e. parent) of the actual value\n",
    "            else:\n",
    "                parents_of_expected_type = get_dbo_superclass(actual_cls)\n",
    "                intersection = [value for value in parents_of_expected_type if value in candidate_class_without_hr]\n",
    "                if len(intersection) > 0:\n",
    "                    parent_found += 0.5\n",
    "                    \n",
    "#             if actual_cls in candidate_class_with_hr:\n",
    "#                 found_with+=1\n",
    "            total_columns+=1\n",
    "    return (round(100*found/total_columns,2),round(100*found_with/total_columns,2),round(100*parent_found/total_columns,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_without_hr'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_candidate['58891288_0_1117541047012405958']['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: elementwise == comparison failed and returning scalar instead; this will raise an error or perform elementwise comparison in the future.\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: FutureWarning: elementwise == comparison failed and returning scalar instead; this will raise an error or perform elementwise comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# first we calculate the max number of candidates across all the columns in the tabular data so we cap the range\n",
    "# max_cand = 0\n",
    "# for file in dict_col_candidate_classes_rank2:\n",
    "#     for col in dict_col_candidate_classes_rank2[file]:\n",
    "        \n",
    "#         candidate_cls = set()\n",
    "#         for element in dict_col_candidate_classes_rank2[file][col]:\n",
    "#             candidate_cls.add(element[0])\n",
    "#         if len(candidate_cls) > max_cand:\n",
    "#             max_cand = len(candidate_cls)\n",
    "\n",
    "# the we increase the size of top list one element at a time until we reac the cap...\n",
    "for i in range(1,16):\n",
    "    (x,y) = (i, lookup_assessment(dict_candidate, i))\n",
    "    results.append((x,y))\n",
    "\n",
    "#... and plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x[0] for x in results],[x[1] for x in results])\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# first we calculate the max number of candidates across all the columns in the tabular data so we cap the range\n",
    "max_cand = 0\n",
    "for file in dict_col_candidate_classes_rank2:\n",
    "    for col in dict_col_candidate_classes_rank2[file]:\n",
    "        \n",
    "        candidate_cls = set()\n",
    "        for element in dict_col_candidate_classes_rank2[file][col]:\n",
    "            candidate_cls.add(element[0])\n",
    "        if len(candidate_cls) > max_cand:\n",
    "            max_cand = len(candidate_cls)\n",
    "\n",
    "# the we increase the size of top list one element at a time until we reac the cap...\n",
    "for i in range(1,16):\n",
    "    (x,y) = (i, lookup_assessment_considering_hr(dict_cand, i))\n",
    "    results.append((x,y))\n",
    "\n",
    "#... and plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x[0] for x in results],[x[1] for x in results])\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# first we calculate the max number of candidates across all the columns in the tabular data so we cap the range\n",
    "max_cand = 0\n",
    "for file in dict_col_candidate_classes:\n",
    "    for col in dict_col_candidate_classes[file]:\n",
    "        \n",
    "        candidate_cls = set()\n",
    "        for element in dict_col_candidate_classes[file][col]:\n",
    "            candidate_cls.add(element[0])\n",
    "        if len(candidate_cls) > max_cand:\n",
    "            max_cand = len(candidate_cls)\n",
    "\n",
    "# the we increase the size of top list one element at a time until we reac the cap...\n",
    "for i in range(1,10):\n",
    "    (x,y) = (i, lookup_assessment(dict_cand, i))\n",
    "    results.append((x,y))\n",
    "\n",
    "#... and plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x[0] for x in results],[x[1] for x in results])\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all folders under the cnn_models root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the directory to where the models are\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "trained_models = list(get_cnn_models(cnn_model_directory))\n",
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cnn_model_directory, candidate_class):\n",
    "    return keras.models.load_model(cnn_model_directory+'\\%s' % candidate_class)\n",
    "\n",
    "# model = load_model(cnn_model_directory, 'Astronaut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
