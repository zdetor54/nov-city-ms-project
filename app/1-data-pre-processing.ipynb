{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lookup candidate entities and classes\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--input_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'data'),\n",
    "    help='Directory of input/output')\n",
    "parser.add_argument(\n",
    "    '--dataset',\n",
    "    type=str,\n",
    "#     default='round_4',\n",
    "    default='round_1',\n",
    "#     default='2020_2T',\n",
    "    help='The folder containing the input data')\n",
    "parser.add_argument(\n",
    "    '--target_filename',\n",
    "    type=str,\n",
    "#     default='CTA_Round4_Targets.csv',\n",
    "    default='CTA_Round1_Targets.csv',\n",
    "#     default='CTA_2T_Targets.csv',    \n",
    "    help='The name of the file that contains the target types for each column')\n",
    "parser.add_argument(\n",
    "    '--gt_filename',\n",
    "    type=str,\n",
    "#     default='CTA_Round4_gt.csv',\n",
    "    default='CTA_Round1_gt.csv',\n",
    "#     default='CTA_2T_gt.csv',\n",
    "    help='The name of the file that contains the ground truth for each column')\n",
    "parser.add_argument(\n",
    "    '--file_type',\n",
    "    type=str,\n",
    "    default='csv',\n",
    "    help='File type')\n",
    "parser.add_argument(\n",
    "    '--lookup_results_rank',\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help='File type')\n",
    "\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "# if not os.path.exists(FLAGS.input_dir):\n",
    "#     os.mkdir(FLAGS.input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files from the input directory\n",
    "def get_data_files(data_folder):\n",
    "    \"\"\"\n",
    "    A function used to get all the csv files from the input directory\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        the folder within  the working directory where the data is located\n",
    "    \"\"\"\n",
    "\n",
    "    files = [] # a list of all filenames, including file extensions, that contain data\n",
    "    csv_files = [] # same list as above but without the file extension\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    csv_files = [f.replace(\".csv\",\"\") for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def get_target_cta_columns(target_config_file, data_folder, csv_files, filter_col = True):\n",
    "    \"\"\"\n",
    "    A function used to get which columns from the csv files need to be considered for the CTA. This is a subset of the file columns ignoring anything that is not an entity\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_config_file : str\n",
    "        the file that contains the target column indices for each file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    filter_col : boolean\n",
    "        a flag to indicate whether we should narrow down the reading of the columns to only those targeted for the CTA task\n",
    "    \"\"\"\n",
    "   \n",
    "    target_col_file = os.path.join(FLAGS.input_dir+data_folder, target_config_file)\n",
    "    df_target_col = pd.read_csv(target_col_file,header=None, names=['filename','column_index'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_target_col = df_target_col.loc[df_target_col['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_target = dict()\n",
    "    \n",
    "    for index,row in df_target_col.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_target:\n",
    "            dict_target[row['filename']]= []\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        if filter_col:\n",
    "            dict_target[row['filename']].append(int(row['column_index']))\n",
    "    \n",
    "    return dict_target\n",
    "\n",
    "def get_ground_truth(file, folder, csv_files):\n",
    "    \"\"\"\n",
    "    A function used to get the ground truths as provided in the setup\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file : str\n",
    "        the file that contains the ground truth for the class of each column in each file\n",
    "    folder : str\n",
    "        the folder that contains the ground truth file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    \"\"\"\n",
    "    \n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "   \n",
    "    filepath = os.path.join(FLAGS.input_dir+folder, file)\n",
    "    df_ground_truth = pd.read_csv(filepath,header=None, names=['filename','column_index', 'class'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_ground_truth = df_ground_truth.loc[df_ground_truth['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_gt = dict()\n",
    "    \n",
    "    for index,row in df_ground_truth.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_gt:\n",
    "            dict_gt[row['filename']]= dict()\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        try:\n",
    "            dict_gt[row['filename']][int(row['column_index'])] = row['class'].split(dbo_prefix)[1]\n",
    "        except:\n",
    "            dict_gt[row['filename']][int(row['column_index'])] = row['class']\n",
    "#             pass\n",
    "#         print(dict_gt)\n",
    "    \n",
    "    return dict_gt\n",
    "\n",
    "def read_data(data_folder, dict_target_col, has_header_row = False):\n",
    "    \"\"\"\n",
    "    A function used to read the data from the csvs in the data_folder only considering the columns that are in the dict_target_col\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    folder : str\n",
    "        the folder that contains the csvs with the tabular data\n",
    "    dict_target_col : dictionary\n",
    "        a dictionary with csv filenames as the key and an array of relevant column indices as a value\n",
    "    has_header_row : boolean\n",
    "        a flag to indicate whether the first row in the csv files needs to be skipped as it is a header\n",
    "    \"\"\"\n",
    "    data = dict()\n",
    "\n",
    "    for file in dict_target_col:\n",
    "        data[file] = dict()\n",
    "        df_data = pd.DataFrame()\n",
    "        df_title = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        filename = file + '.' + FLAGS.file_type\n",
    "        tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "\n",
    "        # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "        if len(dict_target_col[file])>0:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "        else:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "        # add the column headers to the data dictionary\n",
    "        try:\n",
    "            data[file]['column_titles'] = list(df_title.iloc[0,:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        file_element = dict()\n",
    "        for column in df_data.columns:\n",
    "            file_element[column] = list(set(df_data[column])) #without cell value repetition\n",
    "#             file_element[column] = list(df_data[column]) # with cell value repetition \n",
    "        data[file]['data'] = file_element\n",
    "\n",
    "# #         element['dataframe'] = df_data    \n",
    "#         data.append(element)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Setup\n",
    "\n",
    "As part of this initial step we will need to load the data we are going to process as well as the targets we are trying to meet. The data is located in the data folder as follows\n",
    "- round_1:\n",
    "    - gt: the expected outcome (ground truth)\n",
    "    - tables: the tabular data\n",
    "    - targets: the columns / cells we need to consider for the CTA/CEA\n",
    "----\n",
    "Step 1: Get a list of all the csv files in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10579449_0_1681126353774891032',\n",
       " '11833461_1_3811022039809817402',\n",
       " '13719111_1_5719401842463579519',\n",
       " '14067031_0_559833072073397908',\n",
       " '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of csv files with tabular data\n",
    "csv_files = get_data_files('\\\\%s\\\\tables' % FLAGS.dataset)\n",
    "# csv_files = csv_files[:1]\n",
    "csv_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get the columns we need to consider for the CTA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('58891288_0_1117541047012405958', [1, 3]),\n",
       " ('8468806_0_4382447409703007384', [1, 2]),\n",
       " ('50245608_0_871275842592178099', [0, 3, 4]),\n",
       " ('14067031_0_559833072073397908', [1, 7, 5, 0]),\n",
       " ('39759273_0_1427898308030295194', [1, 3])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns we need to consider for the CTA task\n",
    "dict_target_col = get_target_cta_columns(FLAGS.target_filename, '\\\\%s\\\\targets' % FLAGS.dataset, csv_files,True)\n",
    "list(islice(dict_target_col.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the ground truth for all columns in the set of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('58891288_0_1117541047012405958', {1: 'Film', 3: 'Person'}),\n",
       " ('8468806_0_4382447409703007384', {1: 'Lake', 2: 'Country'}),\n",
       " ('50245608_0_871275842592178099', {0: 'Film', 3: 'Person', 4: 'Writer'}),\n",
       " ('14067031_0_559833072073397908',\n",
       "  {1: 'Language', 7: 'Currency', 5: 'City', 0: 'Country'}),\n",
       " ('39759273_0_1427898308030295194', {1: 'Film', 3: 'Person'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = get_ground_truth(FLAGS.gt_filename, '\\\\%s\\\\gt' % FLAGS.dataset, csv_files)\n",
    "list(islice(ground_truth.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "The next step is to load the data from the csv files. We load the data as an array of dictionaries.\n",
    "Each dictionary will have the following structure:<br>\n",
    "{<br>\n",
    "'1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5':{<br>\n",
    "<blockquote>\n",
    "<strong>'column_titles'</strong>: ['Party'],<br>\n",
    "<strong>'data'</strong>: <br>\n",
    "    {<br>\n",
    "    <blockquote>\n",
    "        <strong>0:</strong> ['PC', 'Lib-Dem','SNP','UKIP','Labour','BNP','Conservative','Green']<br>\n",
    "    </blockquote>\n",
    "        }<br>\n",
    "<strong>'gt'</strong>: <br>\n",
    "    {<br>\n",
    "    <blockquote>\n",
    "        <strong>0:</strong> 'Film'<br>\n",
    "        <strong>1:</strong> 'Person'<br>\n",
    "    </blockquote>\n",
    "        }<br>\n",
    "</blockquote>\n",
    "    }<br>\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('\\\\%s\\\\tables' % FLAGS.dataset, dict_target_col,True)\n",
    "\n",
    "def append_gt_to_data(data, ground_truth):\n",
    "    for file in data:\n",
    "        data[file]['gt'] = dict() \n",
    "        for col in data[file]['data']:\n",
    "#             print(col, ground_truth[filename][col])\n",
    "            data[file]['gt'][col] = ground_truth[file][col]\n",
    "        \n",
    "append_gt_to_data(data, ground_truth)\n",
    "\n",
    "with open(('output/data-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbo_sparql_results(query_string):\n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    sparql.setQuery(query_string)\n",
    "    \n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        return qres\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_dbo_classes_sparql(cell):\n",
    "    \n",
    "    classes = list([])\n",
    "\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "#     print(f'###################{cell}########################')\n",
    "    query_string = f'''\n",
    "    SELECT ?class\n",
    "    WHERE {{ dbr:{cell} a ?class.\n",
    "    }}'''\n",
    "\n",
    "#         query_string = f'''\n",
    "#         select distinct ?superclass \n",
    "#         where {{dbr:{cell} rdf:type ?e. \n",
    "#             ?e rdfs:subClassOf* ?superclass.\n",
    "#         FILTER (strstarts(str(?superclass), '{dbo_prefix}'))}}'''\n",
    "\n",
    "#         print(query_string)\n",
    "\n",
    "    qres = dbo_sparql_results(query_string)\n",
    "#         pprint(qres)\n",
    "    try:\n",
    "        for entity_class in qres['results']['bindings']:\n",
    "            if dbo_prefix in entity_class[list(qres['results']['bindings'][1].keys())[0]]['value']:\n",
    "                candicate_class = entity_class[list(qres['results']['bindings'][1].keys())[0]]['value'].split(dbo_prefix)[1]\n",
    "                classes.append(candicate_class)\n",
    "#                 print(candicate_class)\n",
    "    except:\n",
    "        pass\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_dbpedia_classes (query_string, max_hits = 5):\n",
    "    web_api = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=%s&QueryString=%s'\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    entity_classes = dict()\n",
    "    try:\n",
    "        lookup_url = web_api % (max_hits, query_string)\n",
    "#         print(lookup_url)\n",
    "        lookup_res = requests.get(lookup_url,verify=False)\n",
    "        root = ET.fromstring(lookup_res.content)\n",
    "        i=0\n",
    "        for child in root:\n",
    "            i+=1\n",
    "#             print(\"\\n\")\n",
    "            entity = child[1].text.split(dbp_prefix)[1]\n",
    "#             print(entity)\n",
    "            classes = list()\n",
    "            for cc in child[3]:\n",
    "                cls_URI = cc[1].text\n",
    "#                 print(cls_URI)\n",
    "                if dbo_prefix in cls_URI:\n",
    "                    classes.append((cls_URI.split(dbo_prefix)[1]))\n",
    "            \n",
    "            # if no classes have been retrieved from the lookup go to the sparql endpoint to get the classes for the entity\n",
    "            if len(classes) == 0:\n",
    "                classes = get_dbo_classes_sparql(re.escape(entity))\n",
    "                \n",
    "            if len(classes) > 0:\n",
    "                entity_classes[entity] = dict()\n",
    "                entity_classes[entity]['rank'] = i\n",
    "                entity_classes[entity]['candidate_classes'] = classes\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return entity_classes\n",
    "\n",
    "def lookup_cells_in_dbpedia(data,cell_values = dict()):\n",
    "    i = 0\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    size = 0\n",
    "    for file_i in data:\n",
    "        for col in data[file_i]['data']:\n",
    "            for line_j in range(len(data[file_i]['data'][col])):\n",
    "                size+=1\n",
    "                \n",
    "    print(f'The size of the cell values is: {size}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # from tqdm import tqdm\n",
    "    for file_i in tqdm(data):\n",
    "        \n",
    "        if 'parsed_files' not in cell_values.keys():\n",
    "            cell_values['parsed_files'] = list()\n",
    "            \n",
    "        if file_i not in cell_values['parsed_files']:\n",
    "        #     print(data[file_i])\n",
    "            filename = file_i\n",
    "            for col in data[file_i]['data']:\n",
    "                column_index = col\n",
    "        #         print(data[file_i]['data'][col])\n",
    "                print(file_i, col, len(data[file_i]['data'][col]))\n",
    "                for line_j in range(len(data[file_i]['data'][col])):\n",
    "                    i+=1\n",
    "                    cell_value = data[file_i]['data'][col][line_j]\n",
    "                    if cell_value in cell_values.keys():\n",
    "                        cell_values[cell_value]['location'].append((filename,column_index))\n",
    "                    else:\n",
    "                        cell_values[cell_value] = dict()\n",
    "                        cell_values[cell_value]['location'] = [(filename,column_index)]\n",
    "                        try:\n",
    "                            cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value.replace(\"[\",'').replace(\"]\",''),FLAGS.lookup_results_rank)\n",
    "                        except:\n",
    "                            cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value,FLAGS.lookup_results_rank)\n",
    "            cell_values['parsed_files'].append(file_i)\n",
    "            with open('output/cell_values.json', 'w') as fp:\n",
    "                json.dump(cell_values, fp)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"{int(end_time - start_time)//60} min and {int((end_time - start_time)%60)} seconds Elapsed\")\n",
    "    \n",
    "    # Also save the data in json for future runs\n",
    "    with open(('output/cell_values-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(cell_values, fp)\n",
    "    \n",
    "    return cell_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "from pprint import pprint\n",
    "\n",
    "def get_dbo_class_entities_sparql(candidate_class, num_of_results = 10000):\n",
    "    sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "    \n",
    "    ent_list = []\n",
    "\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    \n",
    "#     print(f'###################{candidate_class}########################')\n",
    "    sparql.setQuery(f'''\n",
    "    SELECT ?object\n",
    "    WHERE {{ ?object a dbo:{candidate_class}. }}\n",
    "    ORDER BY RAND()\n",
    "    LIMIT {num_of_results}\n",
    "    ''')\n",
    "    try:\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "        for entity in qres['results']['bindings']:\n",
    "            ent_list.append(entity['object']['value'].split(dbp_prefix)[1])\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lookup cell_values\n",
    "\n",
    "With the data loaded in the *data* dictionary the next step is to lookup the cell values in the DBpedia endpoint and get the canidate classes and entities.\n",
    "Each cell value is only looked up once, however we still keep track of any column it might have appeared in as well as all candidate entities and classes it may have matched to.\n",
    "\n",
    "For this level of analysis we are flexible to store the 5 top lookup results for each cell value (default value for FLAGS.lookup_results_rank).\n",
    "We will then assess the number of classifiers we need to train later and perhaps filter out any candidate classes that only appeared in lower ranks.\n",
    "\n",
    "The outcome of the lookup is stored in the *cell_values* dictionary as follows:\n",
    "\n",
    "{<strong>\"Madagascar\":</strong><br>\n",
    "{\n",
    "<blockquote><strong>\"location\":</strong> [(\"14067031_0_559833072073397908\",0)]\n",
    "            , <br><strong>\"candidate_entities\":</strong><br> \n",
    "                        {\n",
    "                            <blockquote><strong>\"Madagascar\":</strong> <br>{<blockquote><strong>\"rank\":</strong> 1,<br> <strong>\"candidate_classes\":</strong> [\"Place\", \"Country\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"Antananarivo\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 3,<br> <strong>\"candidate_classes\":</strong> [\"Settlement\", \"Place\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"List_of_Madagascar_(franchise)_characters\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 4,<br> <strong>\"candidate_classes\":</strong> [\"FictionalCharacter\", \"Agent\"]</blockquote>},<br>\n",
    "                            <strong>\"Madagascar_national_football_team\"</strong> <br> {<blockquote><strong>\"rank\":</strong> 5,<br> <strong>\"candidate_classes\":</strong> [\"Organisation\", \"SoccerClub\", \"Agent\", \"SportsClub\"]</blockquote>}<br>\n",
    "</blockquote>}<br> \n",
    "</blockquote>},<br>\n",
    "               \n",
    " <strong>\"South Africa\":</strong> {...},<br>\n",
    "  ...<br>\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively load the lookup values previously saved as part of a past lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [03:30<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 min and 30 seconds Elapsed\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "\n",
    "cell_values_directory = os.getcwd()+'\\\\output\\\\'\n",
    "\n",
    "try:\n",
    "    cell_values_json = cell_values_directory + 'cell_values.json'\n",
    "    with open(cell_values_json) as json_file:\n",
    "        cell_values = json.load(json_file)\n",
    "    cell_values = lookup_cells_in_dbpedia(data, cell_values)\n",
    "except:\n",
    "    cell_values = lookup_cells_in_dbpedia(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cell values are fully looked up the next step is to remove the parsed_file key as that was only used to pickup from where we left off if the lookup operation was cut short before parsing all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8703\n",
      "8702\n"
     ]
    }
   ],
   "source": [
    "print(len(list(cell_values.keys())))\n",
    "try:\n",
    "    del cell_values[\"parsed_files\"]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(len(list(cell_values.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(islice(cell_values.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_dbpedia_classes('Indiana Jones',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process that data\n",
    "\n",
    "The next step is to process the data so that we can use them for training the classifiers and also predicting classes. To achieve that we create the following structures:\n",
    "\n",
    "## 3.1 dict_col_candidate_classes\n",
    "This is a dictionary with the following structure <br/>\n",
    "{<strong>'58891288_0_1117541047012405958'</strong>: \n",
    "<br/>{\n",
    "<blockquote><strong>1</strong>: [('PoliticalParty', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                   ('Organisation', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                   ('Agent', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                    ...<br/>\n",
    "                   ('Book', 'The_Bridge_over_the_River_Kwai', 'The Bridge on the River Kwai', 1)]<br>\n",
    "            <strong>2</strong>: [('PoliticalParty', 'Shining_Path', 'The Shining', 5),<br>\n",
    "                    ...<br/>\n",
    "</blockquote>\n",
    "}<br/>\n",
    "<strong>'58891288_0_1117541047012405958'</strong>: {...}<br/>\n",
    "}\n",
    "where each element in the array represents (type, entity, cell value, rank) of all the lookup results for each cell in that column of that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_candidate_classes = dict()\n",
    "threshold = 5\n",
    "\n",
    "for filename in dict_target_col: #later replace with dict_target_col\n",
    "    dict_col_candidate_classes[filename] = dict()\n",
    "    for i in dict_target_col[filename]:\n",
    "        dict_col_candidate_classes[filename][i] = []\n",
    "    for cell_value in cell_values:\n",
    "        try:\n",
    "            column_index = dict(cell_values[cell_value]['location'])[filename]\n",
    "            for candidate_entity in cell_values[cell_value]['candidate_entities']:\n",
    "                rank = cell_values[cell_value]['candidate_entities'][candidate_entity]['rank']\n",
    "                if rank <= threshold:\n",
    "                    for candidate_class in cell_values[cell_value]['candidate_entities'][candidate_entity]['candidate_classes']:\n",
    "                        dict_col_candidate_classes[filename][column_index].append((candidate_class, candidate_entity, cell_value,rank))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "with open(('output/dict_col_candidate_classes-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(dict_col_candidate_classes, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate classses with repetition. means that if a cell value appears multiple times in the column then the relevant class will also be voted for multiple times when appearing in the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the file columns to do some sample testing\n",
    "from itertools import islice\n",
    "\n",
    "# list(islice(dict_col_candidate_classes.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 df_entities\n",
    "\n",
    "A variariation of dict_col_candidate_classes this is a datafra of the lookup results with columns representing:\n",
    "* type\n",
    "* entity\n",
    "* cell_value and \n",
    "* rank\n",
    "\n",
    "of all the lookup results regardless of file / column the cell value appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>entity</th>\n",
       "      <th>cell_value</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MusicGenre</td>\n",
       "      <td>Rock_music</td>\n",
       "      <td>Rocky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TopicalConcept</td>\n",
       "      <td>Rock_music</td>\n",
       "      <td>Rocky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genre</td>\n",
       "      <td>Rock_music</td>\n",
       "      <td>Rocky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Place</td>\n",
       "      <td>Rocky_Mountains</td>\n",
       "      <td>Rocky</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Rocky_Mountains</td>\n",
       "      <td>Rocky</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             type           entity cell_value  rank\n",
       "0      MusicGenre       Rock_music      Rocky     1\n",
       "1  TopicalConcept       Rock_music      Rocky     1\n",
       "2           Genre       Rock_music      Rocky     1\n",
       "3           Place  Rocky_Mountains      Rocky     2\n",
       "4        Mountain  Rocky_Mountains      Rocky     2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type_neighours_pos_neg_samples['Embryology']\n",
    "df_entities = pd.DataFrame()\n",
    "\n",
    "for filename in dict_col_candidate_classes:\n",
    "    for col in dict_col_candidate_classes[filename]:\n",
    "        df_entities = df_entities.append(pd.DataFrame(dict_col_candidate_classes[filename][col], columns=['type', 'entity', 'cell_value', 'rank']))\n",
    "        \n",
    "df_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 type_neighours_pos_neg_samples\n",
    "\n",
    "This is a dictionary that will help create classifiers for the the candidate classes. For each identified class this dictionary will have the following keys:\n",
    "* 'cooccuring_classes': a set of classes that appear in the same columns as this class\n",
    "* 'positive_candidate_entities': a set of all positive candidate entities that have been retrieved from lookups of the tabular data and belong to this class\n",
    "* 'negative_candidate_entities': a set of all negative candidate entities that have been retrieved from lookups of the tabular data and belong to any of the classes in the neighborhood of this one\n",
    "\n",
    "### 3.2.1 Create the dictionary with all classes and populate the coocuring classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to ensure that classes that appear together as types of the same entity are not selected as positive an negative results we need to make sure we take into account those coocurences\n",
    "\n",
    "# First we get a list of all those coocuring classes that appear as type of the same entity\n",
    "cls_list = list()\n",
    "for cell in cell_values:\n",
    "    for entity in cell_values[cell]['candidate_entities']:\n",
    "        cls_list.append(cell_values[cell]['candidate_entities'][entity]['candidate_classes'])\n",
    "len(cls_list)\n",
    "\n",
    "# The we get the unique sets\n",
    "set_of_co_cls = set(tuple(row) for row in cls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_col_candidate_classes['CTRL_DBP_BUS_automobile_manufacturer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_classes(dict_col_candidate_classes):\n",
    "    \n",
    "    candidate_classes = dict()\n",
    "    for file in dict_col_candidate_classes:\n",
    "        for col in (dict_col_candidate_classes[file]):\n",
    "            \n",
    "            neighours = set([])\n",
    "            for cell in dict_col_candidate_classes[file][col]:\n",
    "                neighours.add(cell[0])\n",
    "                if cell[0] not in candidate_classes.keys():\n",
    "                    candidate_classes[cell[0]] = dict()\n",
    "                    candidate_classes[cell[0]]['cooccuring_classes'] = set()\n",
    "                    candidate_classes[cell[0]]['positive_candidate_entities'] = set()\n",
    "                    candidate_classes[cell[0]]['negative_candidate_entities'] = set()\n",
    "                    candidate_classes[cell[0]]['general_positive_entities'] = set()\n",
    "#             print(neighours)\n",
    "            for candidate_class in neighours:\n",
    "                temp = neighours.copy()\n",
    "                temp.remove(candidate_class)\n",
    "#                 print(temp)\n",
    "                candidate_classes[candidate_class]['cooccuring_classes'].update(temp)\n",
    "\n",
    "#             print(file, '--->', col, '--->', neighours)\n",
    "                \n",
    "    return candidate_classes\n",
    "\n",
    "type_neighours_pos_neg_samples = get_candidate_classes(dict_col_candidate_classes)\n",
    "\n",
    "# for key in type_neighours_pos_neg_samples:\n",
    "#     print(f\"Class:{key} with \\t {len(type_neighours_pos_neg_samples[key]['cooccuring_classes'])} neighbouring classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Populate the 'positive_candidate_entities' key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 380/380 [00:03<00:00, 126.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# update the positive samples for each candidate class\n",
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'].update(set(df_entities[df_entities.type == candidate_cls].entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_neighours_pos_neg_samples['Organisation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 380/380 [00:00<00:00, 6145.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure that classes for the same entity are removed from the neighbours\n",
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    for cls_set in set_of_co_cls:\n",
    "        if candidate_cls in cls_set:\n",
    "            type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'] = type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'] - set(cls_set)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Populate the 'negative_candidate_entities' key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 380/380 [11:51<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# update the negative samples for each class\n",
    "\n",
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    for neighbour_cls in type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes']:\n",
    "        type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'].update(set(df_entities[df_entities.type == neighbour_cls].entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Populate the 'general_positive_entities' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 380/380 [03:19<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    limit = len(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities']=set(get_dbo_class_entities_sparql(candidate_cls,min(limit,10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 380/380 [00:00<00:00, 664.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for candidate_cls in tqdm(type_neighours_pos_neg_samples):\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'] = list(type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\n",
    "    type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities'] = list(type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities'])\n",
    "    \n",
    "with open(('output/type_neighours_pos_neg_samples-%s.json' % time.strftime(\"%Y%m%d-%H%M%S\")), 'w') as fp:\n",
    "        json.dump(type_neighours_pos_neg_samples, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the samples from sets to list in order to be able to save the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for candidate_cls in type_neighours_pos_neg_samples:\n",
    "#     print(candidate_cls, len(type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\\\n",
    "#           , len(type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for candidate_cls in type_neighours_pos_neg_samples:\n",
    "#     print(candidate_cls, len(type_neighours_pos_neg_samples[candidate_cls]['cooccuring_classes'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['positive_candidate_entities'])\\\n",
    "#           ,len(type_neighours_pos_neg_samples[candidate_cls]['negative_candidate_entities'])\\\n",
    "#           , len(type_neighours_pos_neg_samples[candidate_cls]['general_positive_entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)\n",
    "    \n",
    "# type_neighours_pos_neg_samples = load_json(cell_values_directory+'type_neighours_pos_neg_samples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_neighours_pos_neg_samples['Company']['general_positive_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dbo_class_entities_sparql('Company',2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with the lookup results for each cell value in the tabular data\n",
    "def load_json(data_json):\n",
    "    with open(data_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'output\\\\'\n",
    "cnn_model_directory = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "\n",
    "# data = load_json(output_folder+'data.json')\n",
    "# dict_col_candidate_classes = load_json(output_folder+'dict_col_candidate_classes.json')\n",
    "type_neighours_pos_neg_samples = load_json(output_folder+'type_neighours_pos_neg_samples.json')\n",
    "# dict_cand = load_json(output_folder+'dict_cand.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cell_value_word_lenght(data):\n",
    "    cell_values = list()\n",
    "\n",
    "    for file in data:\n",
    "        for col in data[file]['data']:\n",
    "            cell_values += data[file]['data'][col]\n",
    "\n",
    "    cell_values = list(set(cell_values))\n",
    "    len(cell_values)\n",
    "\n",
    "    word_seq = list()\n",
    "\n",
    "    for cell_value in cell_values:\n",
    "        value = str(cell_value).replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(value))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "\n",
    "    return len(word_seq) / len(cell_values)\n",
    "\n",
    "# avg_cell_value_word_lenght(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=30,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/nov-city-ms-project/app/w2v_model/enwiki_model'),\n",
    "#     default='/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONLY LOAD ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.text.en import tokenize\n",
    "\n",
    "def generate_synthetic_columns(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            unit.append(ent)\n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = ent.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_samples(pos, neg, pct = 0.5):\n",
    "    if len(pos) <= len(neg):\n",
    "        return pos+[random.choice(pos) for _ in range(math.ceil((len(neg)-len(pos))*pct))], neg\n",
    "    else:\n",
    "        return pos, neg+[random.choice(neg) for _ in range(math.ceil((len(pos)-len(neg))*pct))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(entities_positive, entities_negative):\n",
    "    # embedding\n",
    "    units_positive = generate_synthetic_columns(entities_positive, FLAGS.synthetic_column_size)\n",
    "    units_negative = generate_synthetic_columns(entities_negative, FLAGS.synthetic_column_size)\n",
    "\n",
    "    sequences_positive = list()\n",
    "    for ent_unit in units_positive:\n",
    "        sequences_positive.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "    sequences_negative = list()\n",
    "    for ent_unit in units_negative:\n",
    "        sequences_negative.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "\n",
    "    x = np.zeros((len(sequences_positive) + len(sequences_negative), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for sample_i, sequence in enumerate(sequences_positive + sequences_negative):\n",
    "        x[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    y_positive = np.ones((len(sequences_positive), 1))\n",
    "    y_negative = np.zeros((len(sequences_negative), 1))\n",
    "    y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "    # shuffling\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    return x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the directory of cnn trained models so that the solution can pick up from where it left off in case there are more classifiers to be trained based on the candidate classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_models(directory):\n",
    "    temp = [x[0] for x in os.walk(directory)]\n",
    "    temp.remove(directory)\n",
    "    return set([x.replace(directory+'\\\\','').split('\\\\')[0] for x in temp])\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "\n",
    "batch_size = 32 \n",
    "epochs = 2\n",
    "test_train_split = 0.2\n",
    "\n",
    "def save_model(model, candidate_class):\n",
    "    cwd = os.getcwd()+'\\\\output\\\\cnn_models'\n",
    "\n",
    "    model.save(cwd+'/%s' % candidate_class)\n",
    "\n",
    "loaded_models = get_cnn_models(os.getcwd()+'\\\\output\\\\cnn_models')\n",
    "\n",
    "for candidate_class in tqdm(type_neighours_pos_neg_samples):\n",
    "    if candidate_class not in loaded_models:\n",
    "        print(candidate_class)\n",
    "        # Get the positive and negative samples to train the model\n",
    "        cls_neg_par_entities = list(type_neighours_pos_neg_samples[candidate_class]['negative_candidate_entities'])\n",
    "        cls_pos_gen_entities = list(type_neighours_pos_neg_samples[candidate_class]['general_positive_entities'])\n",
    "        \n",
    "        # had to add this as I was running out of memory\n",
    "        if len(cls_neg_par_entities)>10000:\n",
    "            cls_neg_par_entities = random.sample(cls_neg_par_entities, 10000)\n",
    "            try: \n",
    "                cls_pos_gen_entities = random.sample(cls_pos_gen_entities, 10000)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(len(cls_pos_gen_entities), len(cls_neg_par_entities))\n",
    "        # align the samples to create a balance set\n",
    "        p_ents, n_ents = align_samples(cls_pos_gen_entities, cls_neg_par_entities,1)\n",
    "        \n",
    "        print(len(p_ents), len(n_ents))\n",
    "        \n",
    "        # Create the embeddings using the w2v_model. here the samples are shuffled so we have a mixture of positive and negative samples\n",
    "        X, Y = embedding(p_ents, n_ents)\n",
    "        \n",
    "\n",
    "        dev_sample_index = int(test_train_split * float(X.shape[0]))\n",
    "        X_train, X_dev = X[dev_sample_index:], X[:dev_sample_index]\n",
    "        Y_train, Y_dev = Y[dev_sample_index:], Y[:dev_sample_index]\n",
    "\n",
    "        HEIGHT = X_train.shape[1]\n",
    "        WIDTH = X_train.shape[2]\n",
    "\n",
    "        #Build the model\n",
    "        model = Sequential([\n",
    "            Conv2D(16, 3, padding='same', activation='relu', \n",
    "                   input_shape=(HEIGHT, WIDTH ,1)),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            MaxPooling2D(),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        # print the model architecture\n",
    "    #     model.summary()\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                           batch_size=batch_size, \n",
    "                           epochs=epochs,  \n",
    "                           verbose=2)\n",
    "        # save the model\n",
    "        save_model(model,candidate_class)\n",
    "        tf.keras.backend.clear_session()\n",
    "        clear_output(wait=True)\n",
    "# 11483 11483 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CanadianFootballTeam\n",
    "\n",
    "# WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.294255). Check your callbacks.\n",
    "---2021----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
